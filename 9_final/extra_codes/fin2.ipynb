{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuned_summ_path = \"./models/summ_pt.pt\"\n",
    "prompt_tuned_qna_path = \"./models/qa_pt.pt\"\n",
    "promptuned_trans_model = \"./models/translation_best\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptuned_qa_model = torch.load(prompt_tuned_qna_path)\n",
    "promptuned_summ_model = torch.load(prompt_tuned_summ_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config\n",
    "trans_config = AutoConfig.from_pretrained(promptuned_trans_model)\n",
    "trans_model = GPT2Model.from_pretrained(promptuned_trans_model, config=trans_config)\n",
    "trans_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "promptuned_summ_model = promptuned_summ_model.to(\"cpu\")\n",
    "promptuned_qa_model = promptuned_qa_model.to(\"cpu\")\n",
    "trans_model = trans_model.to(\"cpu\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_summ_feats(encoded_input):\n",
    "        output = promptuned_summ_model(encoded_input)\n",
    "        summ_row = 0\n",
    "        for row in output[1]:\n",
    "            for loc_row in row:\n",
    "                # shape of loc row is 1, a, b, c\n",
    "                # reshape it to 1, b, a*c\n",
    "                reshaped = loc_row.reshape(1, loc_row.shape[2], loc_row.shape[1]*loc_row.shape[3])\n",
    "                #print(summ_row)\n",
    "                if type(summ_row) == int:\n",
    "                    summ_row = reshaped\n",
    "                else:\n",
    "                    summ_row = summ_row + reshaped\n",
    "        return summ_row\n",
    "def get_qa_feats(encoded_input):\n",
    "    output = promptuned_qa_model(encoded_input)\n",
    "    summ_row = 0\n",
    "    for row in output[1]:\n",
    "        for loc_row in row:\n",
    "            # shape of loc row is 1, a, b, c\n",
    "            # reshape it to 1, b, a*c\n",
    "            reshaped = loc_row.reshape(1, loc_row.shape[2], loc_row.shape[1]*loc_row.shape[3])\n",
    "            #print(summ_row)\n",
    "            if type(summ_row) == int:\n",
    "                summ_row = reshaped\n",
    "            else:\n",
    "                summ_row = summ_row + reshaped\n",
    "    return summ_row\n",
    "\n",
    "def get_trans_feats(encoded_input):\n",
    "    output = trans_model(encoded_input)\n",
    "    output = output['last_hidden_state']\n",
    "    return output\n",
    "\n",
    "def get_feats(list_sents, max_length):\n",
    "    summ_reps = []\n",
    "    qa_reps = []\n",
    "    trans_reps = []\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    for sent in tqdm(list_sents, total = len(list_sents)):\n",
    "        input_s = tokenizer.encode(sent,return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")   \n",
    "        prompt = [0, 1, 2]\n",
    "        # convert to tensor\n",
    "        prompt = torch.tensor(prompt)\n",
    "        # add batch dimension 1\n",
    "        prompt = prompt.unsqueeze(0)\n",
    "        input_s = torch.cat((prompt, input_s), 1)\n",
    "        summ_feats = get_summ_feats(input_s)\n",
    "        trans_feats = get_trans_feats(input_s)\n",
    "        qa_feats = get_qa_feats(input_s)\n",
    "        # convert shape from 1, x, 768 to x, 768\n",
    "        summ_feats = summ_feats.squeeze(0)\n",
    "        trans_feats = trans_feats.squeeze(0)\n",
    "        qa_feats = qa_feats.squeeze(0)\n",
    "        summ_feats = np.array(torch.mean(summ_feats, dim=0).detach().numpy())\n",
    "        trans_feats = np.array(torch.mean(trans_feats, dim=0).detach().numpy())\n",
    "        qa_feats = np.array(torch.mean(qa_feats, dim=0).detach().numpy())   \n",
    "        summ_reps.append(summ_feats)\n",
    "        qa_reps.append(qa_feats)\n",
    "        trans_reps.append(trans_feats)\n",
    "    return summ_reps, qa_reps, trans_reps        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the stim and fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi\n"
     ]
    }
   ],
   "source": [
    "# print cwd\n",
    "import os\n",
    "print(os.getcwd())\n",
    "working_dir = \"csai_a5\"\n",
    "# switch to working dir\n",
    "os.chdir(working_dir)\n",
    "\n",
    "parent_dir = \"assignment5\"\n",
    "stim_file = \"stimuli.txt\"\n",
    "subj1 = \"subj1.npy\"\n",
    "subj2 = \"subj2.npy\"\n",
    "# load the stims file\n",
    "import regex as re\n",
    "with open(os.path.join(parent_dir, stim_file), \"r\") as f:\n",
    "    stims = f.readlines()\n",
    "final_stims = []\n",
    "for item in stims:\n",
    "    # remove all numbers and punctuation\n",
    "    item = re.sub(r'[^\\w\\s]','',item)\n",
    "    # remove all numbers\n",
    "    item = re.sub(r'\\d+', '', item)\n",
    "    item = item.lower()\n",
    "    final_stims.append(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 627/627 [00:44<00:00, 14.06it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for sent in final_stims:\n",
    "    length= len(sent.split())\n",
    "    if max_len < length:\n",
    "        max_len  = length\n",
    "print(max_len)\n",
    "max_len = max_len + 5\n",
    "summ_reps, qa_reps, trans_reps = get_feats(final_stims, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing FMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627, 11437) (627, 33792) (627, 17190) (627, 35120) (627, 10791) (627, 31109) (627, 15070) (627, 30594)\n"
     ]
    }
   ],
   "source": [
    "# load fmri data of subj1 and subj2\n",
    "import numpy as np\n",
    "subj1_data = np.load(os.path.join(parent_dir, subj1), allow_pickle=True).item()\n",
    "subj2_data = np.load(os.path.join(parent_dir, subj2), allow_pickle=True).item()\n",
    "\n",
    "subj1_lang = subj1_data[\"language\"]\n",
    "subj1_vis = subj1_data[\"vision\"]\n",
    "subj1_dmn = subj1_data[\"dmn\"]\n",
    "subj1_task = subj1_data[\"task\"]\n",
    "subj2_lang = subj2_data[\"language\"]\n",
    "subj2_vis = subj2_data[\"vision\"]\n",
    "subj2_dmn = subj2_data[\"dmn\"]\n",
    "subj2_task = subj2_data[\"task\"]\n",
    "print(subj1_lang.shape, subj1_vis.shape, subj1_dmn.shape, subj1_task.shape, subj2_lang.shape, subj2_vis.shape, subj2_dmn.shape, subj2_task.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# import R2 as well\n",
    "from sklearn.metrics import r2_score\n",
    "def cos_d(y1, y2):\n",
    "    cos_sim = np.dot(y1, y2)/(np.linalg.norm(y1)*np.linalg.norm(y2))\n",
    "    cos_d = 1 - cos_sim\n",
    "    return cos_d\n",
    "def get_2v2_accuracy(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_cnt = 0\n",
    "    pos_cnt = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(1, N):\n",
    "            yi_test = y_test[i]\n",
    "            yi_pred = y_pred[i]\n",
    "            yj_test = y_test[j]\n",
    "            yj_pred = y_pred[j]\n",
    "            score1 = cos_d(yi_test, yi_pred) + cos_d(yj_test, yj_pred)\n",
    "            score2 = cos_d(yi_test, yj_pred) + cos_d(yj_test, yi_pred)\n",
    "            if score1 < score2:\n",
    "                pos_cnt += 1\n",
    "            tot_cnt += 1\n",
    "    return pos_cnt/tot_cnt\n",
    "\n",
    "def get_avg_corr(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_corr = 0\n",
    "    for i in range(0, N):\n",
    "        corr = np.corrcoef(y_pred[i], y_test[i])[0,1]\n",
    "        tot_corr += corr\n",
    "    return tot_corr/N\n",
    "\n",
    "def decoding_module(X, y):\n",
    "    kf = KFold(n_splits=5)\n",
    "    mse = []\n",
    "    r2 = []\n",
    "    acc = []\n",
    "    acc_scores = []\n",
    "    corr_scores = []\n",
    "    for train_index, test_index in  kf.split(X) :\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = Ridge()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        get_2v2_accuracy(y_pred, y_test)\n",
    "        mse.append(mean_squared_error(y_test, y_pred))\n",
    "        r2.append(r2_score(y_test, y_pred))\n",
    "        acc_scores.append(get_2v2_accuracy(y_pred, y_test))\n",
    "        corr_scores.append(get_avg_corr(y_pred, y_test))\n",
    "    loc_dict = {}\n",
    "    loc_dict[\"MSE\"] = np.mean(mse)\n",
    "    loc_dict[\"R2\"] = np.mean(r2)\n",
    "    loc_dict[\"Accuracy\"] = np.mean(acc_scores)\n",
    "    loc_dict[\"Correlation\"] = np.mean(corr_scores)\n",
    "    return loc_dict\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "subj1_decoding_scores = {}\n",
    "subj2_decoding_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.38096174884382417, 'R2': -0.7733994461504705, 'Accuracy': 0.7547911930363543, 'Correlation': 0.9494759504220255}\n",
      "{'MSE': 0.35968410024048136, 'R2': -0.6744477745129355, 'Accuracy': 0.7529632360471069, 'Correlation': 0.952180624266617}\n",
      "{'MSE': 0.36020446803740624, 'R2': -0.6721452905994902, 'Accuracy': 0.7181337429595495, 'Correlation': 0.952121002882906}\n",
      "{'MSE': 0.31262654452950556, 'R2': -0.4470878815744852, 'Accuracy': 0.740579211469534, 'Correlation': 0.9581947149771614}\n",
      "{'MSE': 0.4139036434330185, 'R2': -0.9271034626312931, 'Accuracy': 0.727136712749616, 'Correlation': 0.9452519543733832}\n",
      "{'MSE': 0.3945455932026454, 'R2': -0.8340644573931988, 'Accuracy': 0.7316608294930875, 'Correlation': 0.9476600053016186}\n",
      "{'MSE': 0.3998100960872937, 'R2': -0.8595109074048943, 'Accuracy': 0.7142044034818229, 'Correlation': 0.947060189286445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [00:39<01:19, 39.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.35077004519126753, 'R2': -0.6250790389973478, 'Accuracy': 0.7126539682539683, 'Correlation': 0.9532297534357799}\n",
      "{'MSE': 0.38096174884382417, 'R2': -0.7733994461504705, 'Accuracy': 0.7547911930363543, 'Correlation': 0.9494759504220255}\n",
      "{'MSE': 0.35968410024048136, 'R2': -0.6744477745129355, 'Accuracy': 0.7529632360471069, 'Correlation': 0.952180624266617}\n",
      "{'MSE': 0.36020446803740624, 'R2': -0.6721452905994902, 'Accuracy': 0.7181337429595495, 'Correlation': 0.952121002882906}\n",
      "{'MSE': 0.31262654452950556, 'R2': -0.4470878815744852, 'Accuracy': 0.740579211469534, 'Correlation': 0.9581947149771614}\n",
      "{'MSE': 0.4139036434330185, 'R2': -0.9271034626312931, 'Accuracy': 0.727136712749616, 'Correlation': 0.9452519543733832}\n",
      "{'MSE': 0.3945455932026454, 'R2': -0.8340644573931988, 'Accuracy': 0.7316608294930875, 'Correlation': 0.9476600053016186}\n",
      "{'MSE': 0.3998100960872937, 'R2': -0.8595109074048943, 'Accuracy': 0.7142044034818229, 'Correlation': 0.947060189286445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 2/3 [01:17<00:38, 38.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.35077004519126753, 'R2': -0.6250790389973478, 'Accuracy': 0.7126539682539683, 'Correlation': 0.9532297534357799}\n",
      "{'MSE': 0.20043854411256662, 'R2': -0.584358836080643, 'Accuracy': 0.7093788018433179, 'Correlation': 0.9990300722133689}\n",
      "{'MSE': 0.1971109401070051, 'R2': -0.5029438282408814, 'Accuracy': 0.7090154633896569, 'Correlation': 0.9990697503991168}\n",
      "{'MSE': 0.21524041502542998, 'R2': -0.49894688489037414, 'Accuracy': 0.6617681515616999, 'Correlation': 0.9989151142739155}\n",
      "{'MSE': 0.19265129045602353, 'R2': -0.3191236483584917, 'Accuracy': 0.697053149001536, 'Correlation': 0.9991093356539972}\n",
      "{'MSE': 0.22888943881375137, 'R2': -0.6966292774174742, 'Accuracy': 0.6606990271377368, 'Correlation': 0.998842165998596}\n",
      "{'MSE': 0.24006601320970833, 'R2': -0.6141613197891107, 'Accuracy': 0.665368561187916, 'Correlation': 0.9988804338023366}\n",
      "{'MSE': 0.22820536648596637, 'R2': -0.6668713014982955, 'Accuracy': 0.6383490015360984, 'Correlation': 0.9988361922414836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [01:55<00:00, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.21055305811577196, 'R2': -0.4823323738101248, 'Accuracy': 0.6451535074244752, 'Correlation': 0.9989869502257841}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_feats = [summ_reps, qa_reps, trans_reps]\n",
    "list_fmris = [subj1_lang, subj1_vis, subj1_dmn, subj1_task, subj2_lang, subj2_vis, subj2_dmn, subj2_task]\n",
    "feats_names = [\"prompt-tuned-summarization\", \"prompt-tuned-question-answering\", \"prompt-tuned-translation\"]\n",
    "fmri_names = ['subj1_lang', 'subj1_vis', 'subj1_dmn', 'subj1_task', 'subj2_lang', 'subj2_vis', 'subj2_dmn', 'subj2_task']\n",
    "for i in tqdm(range(len(list_feats)), total = len(list_feats)):\n",
    "    for j in range(len(list_fmris)):\n",
    "        label = \"decoding \" + feats_names[i] + \" with \" + fmri_names[j]\n",
    "        reps = list_feats[i]\n",
    "        fmris = list_fmris[j]\n",
    "        x = np.array(fmris)\n",
    "        y = np.array(reps)\n",
    "        decoding_scores = decoding_module(x, y)\n",
    "        if j < 4:\n",
    "            subj1_decoding_scores[label] = decoding_scores\n",
    "        else:\n",
    "            subj2_decoding_scores[label] = decoding_scores\n",
    "        print(decoding_scores)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# import R2 as well\n",
    "from sklearn.metrics import r2_score\n",
    "def cos_d(y1, y2):\n",
    "    cos_sim = np.dot(y1, y2)/(np.linalg.norm(y1)*np.linalg.norm(y2))\n",
    "    cos_d = 1 - cos_sim\n",
    "    return cos_d\n",
    "def get_2v2_accuracy(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_cnt = 0\n",
    "    pos_cnt = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(1, N):\n",
    "            yi_test = y_test[i]\n",
    "            yi_pred = y_pred[i]\n",
    "            yj_test = y_test[j]\n",
    "            yj_pred = y_pred[j]\n",
    "            score1 = cos_d(yi_test, yi_pred) + cos_d(yj_test, yj_pred)\n",
    "            score2 = cos_d(yi_test, yj_pred) + cos_d(yj_test, yi_pred)\n",
    "            if score1 < score2:\n",
    "                pos_cnt += 1\n",
    "            tot_cnt += 1\n",
    "    return pos_cnt/tot_cnt\n",
    "\n",
    "def get_avg_corr(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_corr = 0\n",
    "    for i in range(0, N):\n",
    "        corr = np.corrcoef(y_pred[i], y_test[i])[0,1]\n",
    "        tot_corr += corr\n",
    "    return tot_corr/N\n",
    "\n",
    "def encoding_module(X, y):\n",
    "    kf = KFold(n_splits=5)\n",
    "    mse = []\n",
    "    r2 = []\n",
    "    acc = []\n",
    "    acc_scores = []\n",
    "    corr_scores = []\n",
    "    for train_index, test_index in  kf.split(X) :\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = Ridge()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        get_2v2_accuracy(y_pred, y_test)\n",
    "        mse.append(mean_squared_error(y_test, y_pred))\n",
    "        r2.append(r2_score(y_test, y_pred))\n",
    "        acc_scores.append(get_2v2_accuracy(y_pred, y_test))\n",
    "        corr_scores.append(get_avg_corr(y_pred, y_test))\n",
    "    loc_dict = {}\n",
    "    loc_dict[\"MSE\"] = np.mean(mse)\n",
    "    loc_dict[\"R2\"] = np.mean(r2)\n",
    "    loc_dict[\"Accuracy\"] = np.mean(acc_scores)\n",
    "    loc_dict[\"Correlation\"] = np.mean(corr_scores)\n",
    "    return loc_dict\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "subj1_encoding_scores = {}\n",
    "subj2_encoding_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.38096174884382417, 'R2': -0.7733994461504705, 'Accuracy': 0.7547911930363543, 'Correlation': 0.9494759504220255}\n",
      "{'MSE': 0.35968410024048136, 'R2': -0.6744477745129355, 'Accuracy': 0.7529632360471069, 'Correlation': 0.952180624266617}\n",
      "{'MSE': 0.36020446803740624, 'R2': -0.6721452905994902, 'Accuracy': 0.7181337429595495, 'Correlation': 0.952121002882906}\n",
      "{'MSE': 0.31262654452950556, 'R2': -0.4470878815744852, 'Accuracy': 0.740579211469534, 'Correlation': 0.9581947149771614}\n",
      "{'MSE': 0.4139036434330185, 'R2': -0.9271034626312931, 'Accuracy': 0.727136712749616, 'Correlation': 0.9452519543733832}\n",
      "{'MSE': 0.3945455932026454, 'R2': -0.8340644573931988, 'Accuracy': 0.7316608294930875, 'Correlation': 0.9476600053016186}\n",
      "{'MSE': 0.3998100960872937, 'R2': -0.8595109074048943, 'Accuracy': 0.7142044034818229, 'Correlation': 0.947060189286445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [00:37<01:15, 37.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.35077004519126753, 'R2': -0.6250790389973478, 'Accuracy': 0.7126539682539683, 'Correlation': 0.9532297534357799}\n",
      "{'MSE': 0.38096174884382417, 'R2': -0.7733994461504705, 'Accuracy': 0.7547911930363543, 'Correlation': 0.9494759504220255}\n",
      "{'MSE': 0.35968410024048136, 'R2': -0.6744477745129355, 'Accuracy': 0.7529632360471069, 'Correlation': 0.952180624266617}\n",
      "{'MSE': 0.36020446803740624, 'R2': -0.6721452905994902, 'Accuracy': 0.7181337429595495, 'Correlation': 0.952121002882906}\n",
      "{'MSE': 0.31262654452950556, 'R2': -0.4470878815744852, 'Accuracy': 0.740579211469534, 'Correlation': 0.9581947149771614}\n",
      "{'MSE': 0.4139036434330185, 'R2': -0.9271034626312931, 'Accuracy': 0.727136712749616, 'Correlation': 0.9452519543733832}\n",
      "{'MSE': 0.3945455932026454, 'R2': -0.8340644573931988, 'Accuracy': 0.7316608294930875, 'Correlation': 0.9476600053016186}\n",
      "{'MSE': 0.3998100960872937, 'R2': -0.8595109074048943, 'Accuracy': 0.7142044034818229, 'Correlation': 0.947060189286445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 2/3 [01:15<00:37, 37.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.35077004519126753, 'R2': -0.6250790389973478, 'Accuracy': 0.7126539682539683, 'Correlation': 0.9532297534357799}\n",
      "{'MSE': 0.20043854411256662, 'R2': -0.584358836080643, 'Accuracy': 0.7093788018433179, 'Correlation': 0.9990300722133689}\n",
      "{'MSE': 0.1971109401070051, 'R2': -0.5029438282408814, 'Accuracy': 0.7090154633896569, 'Correlation': 0.9990697503991168}\n",
      "{'MSE': 0.21524041502542998, 'R2': -0.49894688489037414, 'Accuracy': 0.6617681515616999, 'Correlation': 0.9989151142739155}\n",
      "{'MSE': 0.19265129045602353, 'R2': -0.3191236483584917, 'Accuracy': 0.697053149001536, 'Correlation': 0.9991093356539972}\n",
      "{'MSE': 0.22888943881375137, 'R2': -0.6966292774174742, 'Accuracy': 0.6606990271377368, 'Correlation': 0.998842165998596}\n",
      "{'MSE': 0.24006601320970833, 'R2': -0.6141613197891107, 'Accuracy': 0.665368561187916, 'Correlation': 0.9988804338023366}\n",
      "{'MSE': 0.22820536648596637, 'R2': -0.6668713014982955, 'Accuracy': 0.6383490015360984, 'Correlation': 0.9988361922414836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [01:53<00:00, 37.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.21055305811577196, 'R2': -0.4823323738101248, 'Accuracy': 0.6451535074244752, 'Correlation': 0.9989869502257841}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_feats = [summ_reps, qa_reps, trans_reps]\n",
    "list_fmris = [subj1_lang, subj1_vis, subj1_dmn, subj1_task, subj2_lang, subj2_vis, subj2_dmn, subj2_task]\n",
    "feats_names = [\"prompt-tuned-summarization\", \"prompt-tuned-question-answering\", \"prompt-tuned-translation\"]\n",
    "fmri_names = ['subj1_lang', 'subj1_vis', 'subj1_dmn', 'subj1_task', 'subj2_lang', 'subj2_vis', 'subj2_dmn', 'subj2_task']\n",
    "for i in tqdm(range(len(list_feats)), total = len(list_feats)):\n",
    "    for j in range(len(list_fmris)):\n",
    "        label = \"encoding \" + feats_names[i] + \" with \" + fmri_names[j]\n",
    "        reps = list_feats[i]\n",
    "        fmris = list_fmris[j]\n",
    "        x = np.array(fmris)\n",
    "        y = np.array(reps)\n",
    "        encoding_scores = encoding_module(x, y)\n",
    "        if j < 4:\n",
    "            subj1_encoding_scores[label] = encoding_scores\n",
    "        else:\n",
    "            subj2_encoding_scores[label] = encoding_scores\n",
    "        print(encoding_scores)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"scores\")\n",
    "import json\n",
    "with open(\"pt_subj1_encoding_scores.json\", \"w\") as f:\n",
    "    json.dump(subj1_encoding_scores, f, indent=4)\n",
    "with open(\"pt_subj2_encoding_scores.json\", \"w\") as f:\n",
    "    json.dump(subj2_encoding_scores, f, indent=4)\n",
    "with open(\"pt_subj1_decoding_scores.json\", \"w\") as f:\n",
    "    json.dump(subj1_decoding_scores, f, indent=4)\n",
    "with open(\"pt_subj2_decoding_scores.json\", \"w\") as f:\n",
    "    json.dump(subj2_decoding_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/csai_a5/scores\n"
     ]
    }
   ],
   "source": [
    "# print cwd\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
