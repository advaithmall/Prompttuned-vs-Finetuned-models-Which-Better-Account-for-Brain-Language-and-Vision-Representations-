{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'https://lumiere-a.akamaihd.net/v1/images/darth-vader-main_4560aff7.jpeg?region=71%2C0%2C1139%2C854'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:28<00:00, 185.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "dataset = load_dataset(\"mnist\")\n",
    "data = []\n",
    "for row in tqdm(dataset['train'], total = len(dataset['train'])):\n",
    "    image = row['image']\n",
    "    label = row['label']\n",
    "    image = image.resize((32, 32))\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = processor(images=image, return_tensors=\"pt\")\n",
    "    image = image['pixel_values']\n",
    "    new_row = [image, label]\n",
    "    data.append(new_row)\n",
    "import random\n",
    "random.shuffle(data)\n",
    "train_split = data[:10000]\n",
    "test_split = data[10000:11000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test, take 10000 train samples and 1000 test samples\n",
    "# randomly shuffle the data list\n",
    "import random\n",
    "random.shuffle(data)\n",
    "train_split = data[:10000]\n",
    "test_split = data[10000:11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for row in data:\n",
    "    labels.append(row[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "class ptuned_VIT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ptuned_VIT, self).__init__()\n",
    "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.embeddings = nn.Embedding(3, 768)\n",
    "        self.classification_layer = nn.Linear(768, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.trainable = [self.embeddings, self.classification_layer]\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        tens = [0,1,2]\n",
    "        tens = torch.tensor(tens)\n",
    "        tens = tens.to(device)\n",
    "        x1 = self.embeddings(tens)\n",
    "        x2 = self.model.embeddings(x)\n",
    "        # change x1 shape from x, 768 to 1, x, 768\n",
    "        x1 = x1.unsqueeze(0)\n",
    "        # concat x1 and x2 along the first dimension\n",
    "        # x1 is 1, 3, 768, make is x2.shape[0], 3, 768\n",
    "        x1 = x1.expand(x2.shape[0], -1, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = self.model.encoder(x)\n",
    "        x = x['last_hidden_state']\n",
    "        x = self.model.pooler(x)\n",
    "        x = self.classification_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "classes = len(set(labels))\n",
    "p_tokens = 3\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# use data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_split, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_split, batch_size=32, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0/313, Loss: 2.2956485748291016, Running Accuracy: 0.125, Current Accuracy 0.125\n",
      "Epoch: 0, Batch: 1/313, Loss: 2.306826114654541, Running Accuracy: 0.09375, Current Accuracy 0.0625\n",
      "Epoch: 0, Batch: 2/313, Loss: 2.2720913887023926, Running Accuracy: 0.13541666666666666, Current Accuracy 0.21875\n",
      "Epoch: 0, Batch: 3/313, Loss: 2.2937982082366943, Running Accuracy: 0.1328125, Current Accuracy 0.125\n",
      "Epoch: 0, Batch: 4/313, Loss: 2.2543506622314453, Running Accuracy: 0.15625, Current Accuracy 0.25\n",
      "Epoch: 0, Batch: 5/313, Loss: 2.258364200592041, Running Accuracy: 0.16145833333333334, Current Accuracy 0.1875\n",
      "Epoch: 0, Batch: 6/313, Loss: 2.2514963150024414, Running Accuracy: 0.17857142857142858, Current Accuracy 0.28125\n",
      "Epoch: 0, Batch: 7/313, Loss: 2.2462456226348877, Running Accuracy: 0.19921875, Current Accuracy 0.34375\n",
      "Epoch: 0, Batch: 8/313, Loss: 2.1920371055603027, Running Accuracy: 0.22916666666666666, Current Accuracy 0.46875\n",
      "Epoch: 0, Batch: 9/313, Loss: 2.1922924518585205, Running Accuracy: 0.253125, Current Accuracy 0.46875\n",
      "Epoch: 0, Batch: 10/313, Loss: 2.154473066329956, Running Accuracy: 0.2784090909090909, Current Accuracy 0.53125\n",
      "Epoch: 0, Batch: 11/313, Loss: 2.1511647701263428, Running Accuracy: 0.3046875, Current Accuracy 0.59375\n",
      "Epoch: 0, Batch: 12/313, Loss: 2.1708199977874756, Running Accuracy: 0.31009615384615385, Current Accuracy 0.375\n",
      "Epoch: 0, Batch: 13/313, Loss: 2.171558380126953, Running Accuracy: 0.32142857142857145, Current Accuracy 0.46875\n",
      "Epoch: 0, Batch: 14/313, Loss: 2.096123218536377, Running Accuracy: 0.33958333333333335, Current Accuracy 0.59375\n",
      "Epoch: 0, Batch: 15/313, Loss: 2.204591751098633, Running Accuracy: 0.33984375, Current Accuracy 0.34375\n",
      "Epoch: 0, Batch: 16/313, Loss: 2.1188902854919434, Running Accuracy: 0.3492647058823529, Current Accuracy 0.5\n",
      "Epoch: 0, Batch: 17/313, Loss: 2.04984188079834, Running Accuracy: 0.3628472222222222, Current Accuracy 0.59375\n",
      "Epoch: 0, Batch: 18/313, Loss: 2.0856094360351562, Running Accuracy: 0.37335526315789475, Current Accuracy 0.5625\n",
      "Epoch: 0, Batch: 19/313, Loss: 2.037781000137329, Running Accuracy: 0.384375, Current Accuracy 0.59375\n",
      "Epoch: 0, Batch: 20/313, Loss: 2.0606114864349365, Running Accuracy: 0.39285714285714285, Current Accuracy 0.5625\n",
      "Epoch: 0, Batch: 21/313, Loss: 1.951629877090454, Running Accuracy: 0.40625, Current Accuracy 0.6875\n",
      "Epoch: 0, Batch: 22/313, Loss: 1.9902284145355225, Running Accuracy: 0.41847826086956524, Current Accuracy 0.6875\n",
      "Epoch: 0, Batch: 23/313, Loss: 1.9884793758392334, Running Accuracy: 0.4283854166666667, Current Accuracy 0.65625\n",
      "Epoch: 0, Batch: 24/313, Loss: 1.9885543584823608, Running Accuracy: 0.4375, Current Accuracy 0.65625\n",
      "Epoch: 0, Batch: 25/313, Loss: 1.938098430633545, Running Accuracy: 0.44591346153846156, Current Accuracy 0.65625\n",
      "Epoch: 0, Batch: 26/313, Loss: 1.8990455865859985, Running Accuracy: 0.4583333333333333, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 27/313, Loss: 1.9063270092010498, Running Accuracy: 0.46986607142857145, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 28/313, Loss: 1.946526288986206, Running Accuracy: 0.48060344827586204, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 29/313, Loss: 1.8857024908065796, Running Accuracy: 0.490625, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 30/313, Loss: 1.7804372310638428, Running Accuracy: 0.5040322580645161, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 31/313, Loss: 1.897330641746521, Running Accuracy: 0.5107421875, Current Accuracy 0.71875\n",
      "Epoch: 0, Batch: 32/313, Loss: 1.8344950675964355, Running Accuracy: 0.5189393939393939, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 33/313, Loss: 1.8391410112380981, Running Accuracy: 0.5284926470588235, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 34/313, Loss: 1.8133426904678345, Running Accuracy: 0.5366071428571428, Current Accuracy 0.8125\n",
      "Epoch: 0, Batch: 35/313, Loss: 1.7462608814239502, Running Accuracy: 0.546875, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 36/313, Loss: 1.7126989364624023, Running Accuracy: 0.558277027027027, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 37/313, Loss: 1.7678773403167725, Running Accuracy: 0.5674342105263158, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 38/313, Loss: 1.686780571937561, Running Accuracy: 0.5777243589743589, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 39/313, Loss: 1.6808924674987793, Running Accuracy: 0.5875, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 40/313, Loss: 1.8129600286483765, Running Accuracy: 0.59375, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 41/313, Loss: 1.81401789188385, Running Accuracy: 0.5989583333333334, Current Accuracy 0.8125\n",
      "Epoch: 0, Batch: 42/313, Loss: 1.7333494424819946, Running Accuracy: 0.6053779069767442, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 43/313, Loss: 1.7469022274017334, Running Accuracy: 0.6115056818181818, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 44/313, Loss: 1.7426284551620483, Running Accuracy: 0.6166666666666667, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 45/313, Loss: 1.824647068977356, Running Accuracy: 0.6202445652173914, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 46/313, Loss: 1.8290208578109741, Running Accuracy: 0.6243351063829787, Current Accuracy 0.8125\n",
      "Epoch: 0, Batch: 47/313, Loss: 1.7288250923156738, Running Accuracy: 0.630859375, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 48/313, Loss: 1.7233622074127197, Running Accuracy: 0.6358418367346939, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 49/313, Loss: 1.7580573558807373, Running Accuracy: 0.641875, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 50/313, Loss: 1.6320008039474487, Running Accuracy: 0.647671568627451, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 51/313, Loss: 1.6595274209976196, Running Accuracy: 0.6514423076923077, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 52/313, Loss: 1.7178609371185303, Running Accuracy: 0.6556603773584906, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 53/313, Loss: 1.7935423851013184, Running Accuracy: 0.6579861111111112, Current Accuracy 0.78125\n",
      "Epoch: 0, Batch: 54/313, Loss: 1.6840943098068237, Running Accuracy: 0.6630681818181818, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 55/313, Loss: 1.6380971670150757, Running Accuracy: 0.6674107142857143, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 56/313, Loss: 1.6228561401367188, Running Accuracy: 0.6726973684210527, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 57/313, Loss: 1.6347323656082153, Running Accuracy: 0.6772629310344828, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 58/313, Loss: 1.6531734466552734, Running Accuracy: 0.6811440677966102, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 59/313, Loss: 1.6668071746826172, Running Accuracy: 0.6854166666666667, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 60/313, Loss: 1.727787971496582, Running Accuracy: 0.6880122950819673, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 61/313, Loss: 1.6315809488296509, Running Accuracy: 0.6910282258064516, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 62/313, Loss: 1.6284668445587158, Running Accuracy: 0.6939484126984127, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 63/313, Loss: 1.550934076309204, Running Accuracy: 0.6982421875, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 64/313, Loss: 1.6838032007217407, Running Accuracy: 0.7019230769230769, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 65/313, Loss: 1.6369858980178833, Running Accuracy: 0.7059659090909091, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 66/313, Loss: 1.7042580842971802, Running Accuracy: 0.7080223880597015, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 67/313, Loss: 1.6756553649902344, Running Accuracy: 0.7104779411764706, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 68/313, Loss: 1.6597257852554321, Running Accuracy: 0.7142210144927537, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 69/313, Loss: 1.6443610191345215, Running Accuracy: 0.7169642857142857, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 70/313, Loss: 1.6228033304214478, Running Accuracy: 0.7196302816901409, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 71/313, Loss: 1.6806949377059937, Running Accuracy: 0.7222222222222222, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 72/313, Loss: 1.5752489566802979, Running Accuracy: 0.7255993150684932, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 73/313, Loss: 1.6591286659240723, Running Accuracy: 0.7280405405405406, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 74/313, Loss: 1.6155062913894653, Running Accuracy: 0.7304166666666667, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 75/313, Loss: 1.5930862426757812, Running Accuracy: 0.7335526315789473, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 76/313, Loss: 1.5701748132705688, Running Accuracy: 0.7366071428571429, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 77/313, Loss: 1.6783349514007568, Running Accuracy: 0.7375801282051282, Current Accuracy 0.8125\n",
      "Epoch: 0, Batch: 78/313, Loss: 1.6523010730743408, Running Accuracy: 0.7397151898734177, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 79/313, Loss: 1.6116161346435547, Running Accuracy: 0.7421875, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 80/313, Loss: 1.583404302597046, Running Accuracy: 0.7445987654320988, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 81/313, Loss: 1.6066687107086182, Running Accuracy: 0.7469512195121951, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 82/313, Loss: 1.6008634567260742, Running Accuracy: 0.7492469879518072, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 83/313, Loss: 1.597649335861206, Running Accuracy: 0.7522321428571429, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 84/313, Loss: 1.619491696357727, Running Accuracy: 0.7540441176470588, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 85/313, Loss: 1.6162892580032349, Running Accuracy: 0.7554505813953488, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 86/313, Loss: 1.6197500228881836, Running Accuracy: 0.757183908045977, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 87/313, Loss: 1.6244421005249023, Running Accuracy: 0.7595880681818182, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 88/313, Loss: 1.5733460187911987, Running Accuracy: 0.7622893258426966, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 89/313, Loss: 1.6242883205413818, Running Accuracy: 0.7642361111111111, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 90/313, Loss: 1.6214206218719482, Running Accuracy: 0.7654532967032966, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 91/313, Loss: 1.6023993492126465, Running Accuracy: 0.766983695652174, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 92/313, Loss: 1.5378326177597046, Running Accuracy: 0.7691532258064516, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 93/313, Loss: 1.6262520551681519, Running Accuracy: 0.770279255319149, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 94/313, Loss: 1.6654213666915894, Running Accuracy: 0.7707236842105263, Current Accuracy 0.8125\n",
      "Epoch: 0, Batch: 95/313, Loss: 1.5987398624420166, Running Accuracy: 0.7724609375, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 96/313, Loss: 1.552290678024292, Running Accuracy: 0.7744845360824743, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 97/313, Loss: 1.605233907699585, Running Accuracy: 0.7761479591836735, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 98/313, Loss: 1.6133679151535034, Running Accuracy: 0.7777777777777778, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 99/313, Loss: 1.6359270811080933, Running Accuracy: 0.779375, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 100/313, Loss: 1.569239854812622, Running Accuracy: 0.780940594059406, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 101/313, Loss: 1.6184278726577759, Running Accuracy: 0.7818627450980392, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 102/313, Loss: 1.5622690916061401, Running Accuracy: 0.783373786407767, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 103/313, Loss: 1.583251953125, Running Accuracy: 0.7845552884615384, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 104/313, Loss: 1.586608648300171, Running Accuracy: 0.7860119047619047, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 105/313, Loss: 1.5357626676559448, Running Accuracy: 0.7877358490566038, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 106/313, Loss: 1.5719648599624634, Running Accuracy: 0.7891355140186916, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 107/313, Loss: 1.5251203775405884, Running Accuracy: 0.7907986111111112, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 108/313, Loss: 1.576696753501892, Running Accuracy: 0.7924311926605505, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 109/313, Loss: 1.6070414781570435, Running Accuracy: 0.7934659090909091, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 110/313, Loss: 1.6249430179595947, Running Accuracy: 0.7944819819819819, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 111/313, Loss: 1.5601400136947632, Running Accuracy: 0.7960379464285714, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 112/313, Loss: 1.6947219371795654, Running Accuracy: 0.7967367256637168, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 113/313, Loss: 1.5878517627716064, Running Accuracy: 0.7979714912280702, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 114/313, Loss: 1.5475094318389893, Running Accuracy: 0.7994565217391304, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 115/313, Loss: 1.5868239402770996, Running Accuracy: 0.8006465517241379, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 116/313, Loss: 1.6203787326812744, Running Accuracy: 0.8018162393162394, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 117/313, Loss: 1.5578011274337769, Running Accuracy: 0.8034957627118644, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 118/313, Loss: 1.5187615156173706, Running Accuracy: 0.8051470588235294, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 119/313, Loss: 1.5357835292816162, Running Accuracy: 0.8067708333333333, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 120/313, Loss: 1.614250659942627, Running Accuracy: 0.8070764462809917, Current Accuracy 0.84375\n",
      "Epoch: 0, Batch: 121/313, Loss: 1.5354143381118774, Running Accuracy: 0.8086577868852459, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 122/313, Loss: 1.5843499898910522, Running Accuracy: 0.8094512195121951, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 123/313, Loss: 1.6062167882919312, Running Accuracy: 0.8102318548387096, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 124/313, Loss: 1.5129330158233643, Running Accuracy: 0.8115, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 125/313, Loss: 1.5626544952392578, Running Accuracy: 0.8122519841269841, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 126/313, Loss: 1.556832194328308, Running Accuracy: 0.8134842519685039, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 127/313, Loss: 1.632534384727478, Running Accuracy: 0.814208984375, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 128/313, Loss: 1.5541402101516724, Running Accuracy: 0.8156492248062015, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 129/313, Loss: 1.5655595064163208, Running Accuracy: 0.8165865384615385, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 130/313, Loss: 1.5744057893753052, Running Accuracy: 0.8175095419847328, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 131/313, Loss: 1.5352295637130737, Running Accuracy: 0.818655303030303, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 132/313, Loss: 1.5583641529083252, Running Accuracy: 0.8195488721804511, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 133/313, Loss: 1.5771383047103882, Running Accuracy: 0.820429104477612, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 134/313, Loss: 1.5481370687484741, Running Accuracy: 0.8212962962962963, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 135/313, Loss: 1.6072094440460205, Running Accuracy: 0.8219209558823529, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 136/313, Loss: 1.5442144870758057, Running Accuracy: 0.823220802919708, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 137/313, Loss: 1.5787668228149414, Running Accuracy: 0.8240489130434783, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 138/313, Loss: 1.5499556064605713, Running Accuracy: 0.8248651079136691, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 139/313, Loss: 1.5635242462158203, Running Accuracy: 0.8258928571428571, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 140/313, Loss: 1.6059373617172241, Running Accuracy: 0.8262411347517731, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 141/313, Loss: 1.589717984199524, Running Accuracy: 0.8272447183098591, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 142/313, Loss: 1.5942167043685913, Running Accuracy: 0.8277972027972028, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 143/313, Loss: 1.6285827159881592, Running Accuracy: 0.8283420138888888, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 144/313, Loss: 1.5206749439239502, Running Accuracy: 0.8293103448275863, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 145/313, Loss: 1.5494457483291626, Running Accuracy: 0.8300513698630136, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 146/313, Loss: 1.5475175380706787, Running Accuracy: 0.8309948979591837, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 147/313, Loss: 1.5205994844436646, Running Accuracy: 0.8319256756756757, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 148/313, Loss: 1.5404506921768188, Running Accuracy: 0.8326342281879194, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 149/313, Loss: 1.5297540426254272, Running Accuracy: 0.8335416666666666, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 150/313, Loss: 1.5548402070999146, Running Accuracy: 0.8344370860927153, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 151/313, Loss: 1.5834856033325195, Running Accuracy: 0.8353207236842105, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 152/313, Loss: 1.4901597499847412, Running Accuracy: 0.8363970588235294, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 153/313, Loss: 1.5471278429031372, Running Accuracy: 0.8370535714285714, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 154/313, Loss: 1.508195400238037, Running Accuracy: 0.8379032258064516, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 155/313, Loss: 1.5410358905792236, Running Accuracy: 0.8385416666666666, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 156/313, Loss: 1.5585821866989136, Running Accuracy: 0.8395700636942676, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 157/313, Loss: 1.5136091709136963, Running Accuracy: 0.8405854430379747, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 158/313, Loss: 1.6082580089569092, Running Accuracy: 0.8411949685534591, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 159/313, Loss: 1.5506887435913086, Running Accuracy: 0.841796875, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 160/313, Loss: 1.5492576360702515, Running Accuracy: 0.842391304347826, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 161/313, Loss: 1.5775176286697388, Running Accuracy: 0.8429783950617284, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 162/313, Loss: 1.6058827638626099, Running Accuracy: 0.8433665644171779, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 163/313, Loss: 1.5330606698989868, Running Accuracy: 0.8439405487804879, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 164/313, Loss: 1.5149036645889282, Running Accuracy: 0.8448863636363636, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 165/313, Loss: 1.6082149744033813, Running Accuracy: 0.8450677710843374, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 166/313, Loss: 1.5223749876022339, Running Accuracy: 0.8458083832335329, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 167/313, Loss: 1.5271070003509521, Running Accuracy: 0.8467261904761905, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 168/313, Loss: 1.639469027519226, Running Accuracy: 0.8468934911242604, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 169/313, Loss: 1.6026707887649536, Running Accuracy: 0.8470588235294118, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 170/313, Loss: 1.5656059980392456, Running Accuracy: 0.8477704678362573, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 171/313, Loss: 1.5669279098510742, Running Accuracy: 0.8482921511627907, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 172/313, Loss: 1.5366084575653076, Running Accuracy: 0.8489884393063584, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 173/313, Loss: 1.5642969608306885, Running Accuracy: 0.8493175287356322, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 174/313, Loss: 1.5124815702438354, Running Accuracy: 0.85, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 175/313, Loss: 1.5288851261138916, Running Accuracy: 0.8504971590909091, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 176/313, Loss: 1.524560570716858, Running Accuracy: 0.8511652542372882, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 177/313, Loss: 1.5783166885375977, Running Accuracy: 0.8514747191011236, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 178/313, Loss: 1.5120024681091309, Running Accuracy: 0.8521298882681564, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 179/313, Loss: 1.5437307357788086, Running Accuracy: 0.8526041666666667, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 180/313, Loss: 1.5874590873718262, Running Accuracy: 0.8532458563535912, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 181/313, Loss: 1.5650272369384766, Running Accuracy: 0.8535370879120879, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 182/313, Loss: 1.5342295169830322, Running Accuracy: 0.8539959016393442, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 183/313, Loss: 1.5273668766021729, Running Accuracy: 0.8546195652173914, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 184/313, Loss: 1.5900514125823975, Running Accuracy: 0.8550675675675675, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 185/313, Loss: 1.4832407236099243, Running Accuracy: 0.8558467741935484, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 186/313, Loss: 1.583258032798767, Running Accuracy: 0.8561163101604278, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 187/313, Loss: 1.5472965240478516, Running Accuracy: 0.8565492021276596, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 188/313, Loss: 1.6458584070205688, Running Accuracy: 0.8563161375661376, Current Accuracy 0.8125\n",
      "Epoch: 0, Batch: 189/313, Loss: 1.5057601928710938, Running Accuracy: 0.8570723684210526, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 190/313, Loss: 1.5795047283172607, Running Accuracy: 0.8574934554973822, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 191/313, Loss: 1.4961893558502197, Running Accuracy: 0.8582356770833334, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 192/313, Loss: 1.549357295036316, Running Accuracy: 0.8586463730569949, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 193/313, Loss: 1.5117987394332886, Running Accuracy: 0.8592139175257731, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 194/313, Loss: 1.5629991292953491, Running Accuracy: 0.8596153846153847, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 195/313, Loss: 1.5587468147277832, Running Accuracy: 0.8600127551020408, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 196/313, Loss: 1.5040227174758911, Running Accuracy: 0.8607233502538071, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 197/313, Loss: 1.5557931661605835, Running Accuracy: 0.8611111111111112, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 198/313, Loss: 1.5513103008270264, Running Accuracy: 0.8614949748743719, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 199/313, Loss: 1.5219221115112305, Running Accuracy: 0.86203125, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 200/313, Loss: 1.4915145635604858, Running Accuracy: 0.8627176616915423, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 201/313, Loss: 1.4993033409118652, Running Accuracy: 0.8632425742574258, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 202/313, Loss: 1.4915531873703003, Running Accuracy: 0.8639162561576355, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 203/313, Loss: 1.5750924348831177, Running Accuracy: 0.8639705882352942, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 204/313, Loss: 1.492466926574707, Running Accuracy: 0.8646341463414634, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 205/313, Loss: 1.5485848188400269, Running Accuracy: 0.8651395631067961, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 206/313, Loss: 1.6114202737808228, Running Accuracy: 0.8653381642512077, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 207/313, Loss: 1.5820895433425903, Running Accuracy: 0.8656850961538461, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 208/313, Loss: 1.5326694250106812, Running Accuracy: 0.8661782296650717, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 209/313, Loss: 1.5257599353790283, Running Accuracy: 0.8666666666666667, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 210/313, Loss: 1.545708179473877, Running Accuracy: 0.8671504739336493, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 211/313, Loss: 1.4982908964157104, Running Accuracy: 0.8677771226415094, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 212/313, Loss: 1.5740582942962646, Running Accuracy: 0.8679577464788732, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 213/313, Loss: 1.524906873703003, Running Accuracy: 0.868428738317757, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 214/313, Loss: 1.549208402633667, Running Accuracy: 0.86875, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 215/313, Loss: 1.5639770030975342, Running Accuracy: 0.8692129629629629, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 216/313, Loss: 1.5411192178726196, Running Accuracy: 0.8696716589861752, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 217/313, Loss: 1.4865772724151611, Running Accuracy: 0.870269495412844, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 218/313, Loss: 1.5359399318695068, Running Accuracy: 0.8705764840182648, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 219/313, Loss: 1.5030568838119507, Running Accuracy: 0.8710227272727272, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 220/313, Loss: 1.5036529302597046, Running Accuracy: 0.8714649321266968, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 221/313, Loss: 1.5886131525039673, Running Accuracy: 0.8716216216216216, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 222/313, Loss: 1.5027281045913696, Running Accuracy: 0.8721973094170403, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 223/313, Loss: 1.5506898164749146, Running Accuracy: 0.8723493303571429, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 224/313, Loss: 1.5791809558868408, Running Accuracy: 0.8723611111111111, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 225/313, Loss: 1.4949827194213867, Running Accuracy: 0.8729258849557522, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 226/313, Loss: 1.566475510597229, Running Accuracy: 0.8730726872246696, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 227/313, Loss: 1.521146297454834, Running Accuracy: 0.8734923245614035, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 228/313, Loss: 1.6213351488113403, Running Accuracy: 0.8734989082969432, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 229/313, Loss: 1.5212363004684448, Running Accuracy: 0.8739130434782608, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 230/313, Loss: 1.520668864250183, Running Accuracy: 0.8744588744588745, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 231/313, Loss: 1.5537488460540771, Running Accuracy: 0.8748653017241379, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 232/313, Loss: 1.5660948753356934, Running Accuracy: 0.8752682403433476, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 233/313, Loss: 1.5102320909500122, Running Accuracy: 0.8756677350427351, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 234/313, Loss: 1.5023820400238037, Running Accuracy: 0.8760638297872341, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 235/313, Loss: 1.539516806602478, Running Accuracy: 0.8764565677966102, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 236/313, Loss: 1.5608549118041992, Running Accuracy: 0.8768459915611815, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 237/313, Loss: 1.5574512481689453, Running Accuracy: 0.8772321428571429, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 238/313, Loss: 1.5211161375045776, Running Accuracy: 0.8776150627615062, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 239/313, Loss: 1.5566850900650024, Running Accuracy: 0.8778645833333333, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 240/313, Loss: 1.5826655626296997, Running Accuracy: 0.8779823651452282, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 241/313, Loss: 1.5435250997543335, Running Accuracy: 0.8783574380165289, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 242/313, Loss: 1.4853636026382446, Running Accuracy: 0.878858024691358, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 243/313, Loss: 1.5855780839920044, Running Accuracy: 0.8789702868852459, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 244/313, Loss: 1.5623974800109863, Running Accuracy: 0.8789540816326531, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 245/313, Loss: 1.5786395072937012, Running Accuracy: 0.8790650406504065, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 246/313, Loss: 1.572866439819336, Running Accuracy: 0.8793016194331984, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 247/313, Loss: 1.4991706609725952, Running Accuracy: 0.8797883064516129, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 248/313, Loss: 1.5241923332214355, Running Accuracy: 0.8801455823293173, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 249/313, Loss: 1.591489553451538, Running Accuracy: 0.88025, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 250/313, Loss: 1.5249062776565552, Running Accuracy: 0.8806025896414342, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 251/313, Loss: 1.6008857488632202, Running Accuracy: 0.8807043650793651, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 252/313, Loss: 1.588407278060913, Running Accuracy: 0.8808053359683794, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 253/313, Loss: 1.511163592338562, Running Accuracy: 0.8811515748031497, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 254/313, Loss: 1.5204561948776245, Running Accuracy: 0.8814950980392157, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 255/313, Loss: 1.5583266019821167, Running Accuracy: 0.8817138671875, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 256/313, Loss: 1.527062177658081, Running Accuracy: 0.8821741245136187, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 257/313, Loss: 1.490025281906128, Running Accuracy: 0.8826308139534884, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 258/313, Loss: 1.5338393449783325, Running Accuracy: 0.8828426640926641, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 259/313, Loss: 1.4732478857040405, Running Accuracy: 0.8832932692307692, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 260/313, Loss: 1.5275286436080933, Running Accuracy: 0.8835009578544061, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 261/313, Loss: 1.600161075592041, Running Accuracy: 0.8834685114503816, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 262/313, Loss: 1.5250531435012817, Running Accuracy: 0.8837927756653993, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 263/313, Loss: 1.5419824123382568, Running Accuracy: 0.8841145833333334, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 264/313, Loss: 1.5051761865615845, Running Accuracy: 0.8845518867924528, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 265/313, Loss: 1.5548605918884277, Running Accuracy: 0.8848684210526315, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 266/313, Loss: 1.5227994918823242, Running Accuracy: 0.8851825842696629, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 267/313, Loss: 1.570831298828125, Running Accuracy: 0.8853777985074627, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 268/313, Loss: 1.5107080936431885, Running Accuracy: 0.8856877323420075, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 269/313, Loss: 1.5463926792144775, Running Accuracy: 0.8858796296296296, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 270/313, Loss: 1.5020108222961426, Running Accuracy: 0.8863007380073801, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 271/313, Loss: 1.5787667036056519, Running Accuracy: 0.8862591911764706, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 272/313, Loss: 1.6001396179199219, Running Accuracy: 0.8862179487179487, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 273/313, Loss: 1.565490484237671, Running Accuracy: 0.8862910583941606, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 274/313, Loss: 1.5110678672790527, Running Accuracy: 0.8867045454545455, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 275/313, Loss: 1.55000901222229, Running Accuracy: 0.8867753623188406, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 276/313, Loss: 1.529270887374878, Running Accuracy: 0.8869584837545126, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 277/313, Loss: 1.5294129848480225, Running Accuracy: 0.8873651079136691, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 278/313, Loss: 1.533888578414917, Running Accuracy: 0.8875448028673835, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 279/313, Loss: 1.5748212337493896, Running Accuracy: 0.8877232142857143, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 280/313, Loss: 1.5948989391326904, Running Accuracy: 0.8880115658362989, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 281/313, Loss: 1.5199265480041504, Running Accuracy: 0.8882978723404256, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 282/313, Loss: 1.5195690393447876, Running Accuracy: 0.8885821554770318, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 283/313, Loss: 1.4979612827301025, Running Accuracy: 0.8889744718309859, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 284/313, Loss: 1.5138179063796997, Running Accuracy: 0.8893640350877193, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 285/313, Loss: 1.5340200662612915, Running Accuracy: 0.8895323426573427, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 286/313, Loss: 1.5121369361877441, Running Accuracy: 0.889808362369338, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 287/313, Loss: 1.5620355606079102, Running Accuracy: 0.8897569444444444, Current Accuracy 0.875\n",
      "Epoch: 0, Batch: 288/313, Loss: 1.5367518663406372, Running Accuracy: 0.8899221453287197, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 289/313, Loss: 1.513360619544983, Running Accuracy: 0.8901939655172414, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 290/313, Loss: 1.5231821537017822, Running Accuracy: 0.8904639175257731, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 291/313, Loss: 1.5517487525939941, Running Accuracy: 0.890625, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 292/313, Loss: 1.5434253215789795, Running Accuracy: 0.8907849829351536, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 293/313, Loss: 1.5547938346862793, Running Accuracy: 0.8909438775510204, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 294/313, Loss: 1.5797005891799927, Running Accuracy: 0.8911016949152543, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 295/313, Loss: 1.538446307182312, Running Accuracy: 0.8913640202702703, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 296/313, Loss: 1.5134342908859253, Running Accuracy: 0.8916245791245792, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 297/313, Loss: 1.547066569328308, Running Accuracy: 0.8917785234899329, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 298/313, Loss: 1.5629521608352661, Running Accuracy: 0.8918269230769231, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 299/313, Loss: 1.579975962638855, Running Accuracy: 0.891875, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 300/313, Loss: 1.5526701211929321, Running Accuracy: 0.8920265780730897, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 301/313, Loss: 1.5107659101486206, Running Accuracy: 0.8922806291390728, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 302/313, Loss: 1.5506114959716797, Running Accuracy: 0.8924298679867987, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 303/313, Loss: 1.4773844480514526, Running Accuracy: 0.8927837171052632, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 304/313, Loss: 1.5402318239212036, Running Accuracy: 0.8930327868852459, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 305/313, Loss: 1.5515928268432617, Running Accuracy: 0.8930759803921569, Current Accuracy 0.90625\n",
      "Epoch: 0, Batch: 306/313, Loss: 1.5140410661697388, Running Accuracy: 0.8933224755700325, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 307/313, Loss: 1.5274605751037598, Running Accuracy: 0.8935673701298701, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 308/313, Loss: 1.520223617553711, Running Accuracy: 0.8938106796116505, Current Accuracy 0.96875\n",
      "Epoch: 0, Batch: 309/313, Loss: 1.4883173704147339, Running Accuracy: 0.8941532258064516, Current Accuracy 1.0\n",
      "Epoch: 0, Batch: 310/313, Loss: 1.5301343202590942, Running Accuracy: 0.8942926045016077, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 311/313, Loss: 1.5282905101776123, Running Accuracy: 0.8944310897435898, Current Accuracy 0.9375\n",
      "Epoch: 0, Batch: 312/313, Loss: 1.5355457067489624, Running Accuracy: 0.8947683706070287, Current Accuracy 1.0\n",
      "Epoch: 0, Test Accuracy: 0.90625\n",
      "Epoch: 0, Test Accuracy: 0.921875\n",
      "Epoch: 0, Test Accuracy: 0.9479166666666666\n",
      "Epoch: 0, Test Accuracy: 0.9296875\n",
      "Epoch: 0, Test Accuracy: 0.9375\n",
      "Epoch: 0, Test Accuracy: 0.9427083333333334\n",
      "Epoch: 0, Test Accuracy: 0.9419642857142857\n",
      "Epoch: 0, Test Accuracy: 0.94921875\n",
      "Epoch: 0, Test Accuracy: 0.9513888888888888\n",
      "Epoch: 0, Test Accuracy: 0.953125\n",
      "Epoch: 0, Test Accuracy: 0.9517045454545454\n",
      "Epoch: 0, Test Accuracy: 0.953125\n",
      "Epoch: 0, Test Accuracy: 0.9543269230769231\n",
      "Epoch: 0, Test Accuracy: 0.9575892857142857\n",
      "Epoch: 0, Test Accuracy: 0.9541666666666667\n",
      "Epoch: 0, Test Accuracy: 0.95703125\n",
      "Epoch: 0, Test Accuracy: 0.9595588235294118\n",
      "Epoch: 0, Test Accuracy: 0.9600694444444444\n",
      "Epoch: 0, Test Accuracy: 0.9588815789473685\n",
      "Epoch: 0, Test Accuracy: 0.9609375\n",
      "Epoch: 0, Test Accuracy: 0.9613095238095238\n",
      "Epoch: 0, Test Accuracy: 0.9559659090909091\n",
      "Epoch: 0, Test Accuracy: 0.9551630434782609\n",
      "Epoch: 0, Test Accuracy: 0.9518229166666666\n",
      "Epoch: 0, Test Accuracy: 0.9525\n",
      "Epoch: 0, Test Accuracy: 0.9519230769230769\n",
      "Epoch: 0, Test Accuracy: 0.9513888888888888\n",
      "Epoch: 0, Test Accuracy: 0.953125\n",
      "Epoch: 0, Test Accuracy: 0.9525862068965517\n",
      "Epoch: 0, Test Accuracy: 0.953125\n",
      "Epoch: 0, Test Accuracy: 0.9536290322580645\n",
      "Epoch: 0, Test Accuracy: 0.955078125\n",
      "Epoch: 1, Batch: 0/313, Loss: 1.5024539232254028, Running Accuracy: 0.9564393939393939, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 1/313, Loss: 1.501941204071045, Running Accuracy: 0.9577205882352942, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 2/313, Loss: 1.5202993154525757, Running Accuracy: 0.9571428571428572, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 3/313, Loss: 1.496169924736023, Running Accuracy: 0.9574652777777778, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 4/313, Loss: 1.5063961744308472, Running Accuracy: 0.9577702702702703, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 5/313, Loss: 1.5210304260253906, Running Accuracy: 0.9580592105263158, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 6/313, Loss: 1.5067119598388672, Running Accuracy: 0.9591346153846154, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 7/313, Loss: 1.5441410541534424, Running Accuracy: 0.95859375, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 8/313, Loss: 1.5317822694778442, Running Accuracy: 0.958079268292683, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 9/313, Loss: 1.5180227756500244, Running Accuracy: 0.9575892857142857, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 10/313, Loss: 1.544341802597046, Running Accuracy: 0.9578488372093024, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 11/313, Loss: 1.4835777282714844, Running Accuracy: 0.9588068181818182, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 12/313, Loss: 1.551302194595337, Running Accuracy: 0.9583333333333334, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 13/313, Loss: 1.5835744142532349, Running Accuracy: 0.9565217391304348, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 14/313, Loss: 1.5161402225494385, Running Accuracy: 0.956781914893617, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 15/313, Loss: 1.52888822555542, Running Accuracy: 0.95703125, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 16/313, Loss: 1.514847993850708, Running Accuracy: 0.9566326530612245, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 17/313, Loss: 1.5331120491027832, Running Accuracy: 0.956875, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 18/313, Loss: 1.4760230779647827, Running Accuracy: 0.9577205882352942, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 19/313, Loss: 1.5652028322219849, Running Accuracy: 0.9567307692307693, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 20/313, Loss: 1.532305121421814, Running Accuracy: 0.9569575471698113, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 21/313, Loss: 1.5178556442260742, Running Accuracy: 0.9571759259259259, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 22/313, Loss: 1.4818565845489502, Running Accuracy: 0.9579545454545455, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 23/313, Loss: 1.495033860206604, Running Accuracy: 0.9587053571428571, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 24/313, Loss: 1.4734508991241455, Running Accuracy: 0.9594298245614035, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 25/313, Loss: 1.5417428016662598, Running Accuracy: 0.9585129310344828, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 26/313, Loss: 1.5092912912368774, Running Accuracy: 0.9586864406779662, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 27/313, Loss: 1.4955240488052368, Running Accuracy: 0.9588541666666667, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 28/313, Loss: 1.5237118005752563, Running Accuracy: 0.9585040983606558, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 29/313, Loss: 1.5291999578475952, Running Accuracy: 0.9586693548387096, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 30/313, Loss: 1.4960052967071533, Running Accuracy: 0.9593253968253969, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 31/313, Loss: 1.516348123550415, Running Accuracy: 0.95947265625, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 32/313, Loss: 1.4788756370544434, Running Accuracy: 0.9600961538461539, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 33/313, Loss: 1.500220775604248, Running Accuracy: 0.9602272727272727, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 34/313, Loss: 1.4819847345352173, Running Accuracy: 0.960820895522388, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 35/313, Loss: 1.506410002708435, Running Accuracy: 0.9609375, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 36/313, Loss: 1.4919943809509277, Running Accuracy: 0.9610507246376812, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 37/313, Loss: 1.515484094619751, Running Accuracy: 0.9611607142857143, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 38/313, Loss: 1.4783061742782593, Running Accuracy: 0.9617077464788732, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 39/313, Loss: 1.5019876956939697, Running Accuracy: 0.9618055555555556, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 40/313, Loss: 1.5413029193878174, Running Accuracy: 0.961472602739726, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 41/313, Loss: 1.5306652784347534, Running Accuracy: 0.9611486486486487, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 42/313, Loss: 1.5424208641052246, Running Accuracy: 0.9608333333333333, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 43/313, Loss: 1.4810899496078491, Running Accuracy: 0.9613486842105263, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 44/313, Loss: 1.5010632276535034, Running Accuracy: 0.9614448051948052, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 45/313, Loss: 1.600807547569275, Running Accuracy: 0.9603365384615384, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 46/313, Loss: 1.5228755474090576, Running Accuracy: 0.9604430379746836, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 47/313, Loss: 1.5093783140182495, Running Accuracy: 0.960546875, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 48/313, Loss: 1.5557539463043213, Running Accuracy: 0.9602623456790124, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 49/313, Loss: 1.519046425819397, Running Accuracy: 0.9603658536585366, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 50/313, Loss: 1.5241159200668335, Running Accuracy: 0.9600903614457831, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 51/313, Loss: 1.504912257194519, Running Accuracy: 0.9601934523809523, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 52/313, Loss: 1.534816861152649, Running Accuracy: 0.9599264705882353, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 53/313, Loss: 1.5100533962249756, Running Accuracy: 0.9603924418604651, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 54/313, Loss: 1.501849889755249, Running Accuracy: 0.9608477011494253, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 55/313, Loss: 1.556536316871643, Running Accuracy: 0.9605823863636364, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 56/313, Loss: 1.4705309867858887, Running Accuracy: 0.9610252808988764, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 57/313, Loss: 1.4833245277404785, Running Accuracy: 0.9614583333333333, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 58/313, Loss: 1.5138596296310425, Running Accuracy: 0.9615384615384616, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 59/313, Loss: 1.4933184385299683, Running Accuracy: 0.9619565217391305, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 60/313, Loss: 1.5268690586090088, Running Accuracy: 0.9620295698924731, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 61/313, Loss: 1.5227608680725098, Running Accuracy: 0.9624335106382979, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 62/313, Loss: 1.5106837749481201, Running Accuracy: 0.9625, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 63/313, Loss: 1.5212329626083374, Running Accuracy: 0.9625651041666666, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 64/313, Loss: 1.5363932847976685, Running Accuracy: 0.9623067010309279, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 65/313, Loss: 1.493255615234375, Running Accuracy: 0.9626913265306123, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 66/313, Loss: 1.5184211730957031, Running Accuracy: 0.9627525252525253, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 67/313, Loss: 1.5356725454330444, Running Accuracy: 0.9625, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 68/313, Loss: 1.537001609802246, Running Accuracy: 0.9625618811881188, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 69/313, Loss: 1.498597264289856, Running Accuracy: 0.9626225490196079, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 70/313, Loss: 1.488229751586914, Running Accuracy: 0.9626820388349514, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 71/313, Loss: 1.5863975286483765, Running Accuracy: 0.9618389423076923, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 72/313, Loss: 1.5781571865081787, Running Accuracy: 0.9613095238095238, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 73/313, Loss: 1.5004571676254272, Running Accuracy: 0.9613797169811321, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 74/313, Loss: 1.540756344795227, Running Accuracy: 0.9614485981308412, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 75/313, Loss: 1.603590965270996, Running Accuracy: 0.9609375, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 76/313, Loss: 1.5042026042938232, Running Accuracy: 0.9610091743119266, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 77/313, Loss: 1.540615439414978, Running Accuracy: 0.9605113636363637, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 78/313, Loss: 1.4767818450927734, Running Accuracy: 0.9608671171171171, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 79/313, Loss: 1.5121456384658813, Running Accuracy: 0.9609375, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 80/313, Loss: 1.480026125907898, Running Accuracy: 0.9612831858407079, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 81/313, Loss: 1.4687526226043701, Running Accuracy: 0.9616228070175439, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 82/313, Loss: 1.503049373626709, Running Accuracy: 0.9616847826086956, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 83/313, Loss: 1.4715014696121216, Running Accuracy: 0.9620150862068966, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 84/313, Loss: 1.4946552515029907, Running Accuracy: 0.9623397435897436, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 85/313, Loss: 1.500910758972168, Running Accuracy: 0.9623940677966102, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 86/313, Loss: 1.4976881742477417, Running Accuracy: 0.9624474789915967, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 87/313, Loss: 1.544692039489746, Running Accuracy: 0.9622395833333334, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 88/313, Loss: 1.542993187904358, Running Accuracy: 0.9620351239669421, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 89/313, Loss: 1.5100631713867188, Running Accuracy: 0.9620901639344263, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 90/313, Loss: 1.517926812171936, Running Accuracy: 0.9623983739837398, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 91/313, Loss: 1.4696906805038452, Running Accuracy: 0.9627016129032258, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 92/313, Loss: 1.557644248008728, Running Accuracy: 0.9625, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 93/313, Loss: 1.5213141441345215, Running Accuracy: 0.9625496031746031, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 94/313, Loss: 1.499117374420166, Running Accuracy: 0.9628444881889764, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 95/313, Loss: 1.5009734630584717, Running Accuracy: 0.962890625, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 96/313, Loss: 1.5412638187408447, Running Accuracy: 0.9624515503875969, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 97/313, Loss: 1.5207152366638184, Running Accuracy: 0.9625, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 98/313, Loss: 1.5696574449539185, Running Accuracy: 0.9620706106870229, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 99/313, Loss: 1.4689587354660034, Running Accuracy: 0.9623579545454546, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 100/313, Loss: 1.591809868812561, Running Accuracy: 0.9619360902255639, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 101/313, Loss: 1.5049314498901367, Running Accuracy: 0.9622201492537313, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 102/313, Loss: 1.5551797151565552, Running Accuracy: 0.9618055555555556, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 103/313, Loss: 1.4761133193969727, Running Accuracy: 0.9620863970588235, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 104/313, Loss: 1.4843499660491943, Running Accuracy: 0.9623631386861314, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 105/313, Loss: 1.5047508478164673, Running Accuracy: 0.9624094202898551, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 106/313, Loss: 1.5240882635116577, Running Accuracy: 0.9622302158273381, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 107/313, Loss: 1.4890422821044922, Running Accuracy: 0.9622767857142858, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 108/313, Loss: 1.4989246129989624, Running Accuracy: 0.9625443262411347, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 109/313, Loss: 1.5100677013397217, Running Accuracy: 0.9625880281690141, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 110/313, Loss: 1.4748371839523315, Running Accuracy: 0.9628496503496503, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 111/313, Loss: 1.470831036567688, Running Accuracy: 0.9631076388888888, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 112/313, Loss: 1.5299848318099976, Running Accuracy: 0.9629310344827586, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 113/313, Loss: 1.4993335008621216, Running Accuracy: 0.962970890410959, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 114/313, Loss: 1.4960678815841675, Running Accuracy: 0.9632227891156463, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 115/313, Loss: 1.5475683212280273, Running Accuracy: 0.9628378378378378, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 116/313, Loss: 1.5098059177398682, Running Accuracy: 0.9628775167785235, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 117/313, Loss: 1.5158172845840454, Running Accuracy: 0.9629166666666666, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 118/313, Loss: 1.518993854522705, Running Accuracy: 0.9631622516556292, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 119/313, Loss: 1.505216360092163, Running Accuracy: 0.9631990131578947, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 120/313, Loss: 1.596466302871704, Running Accuracy: 0.9626225490196079, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 121/313, Loss: 1.5284152030944824, Running Accuracy: 0.9624594155844156, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 122/313, Loss: 1.4799259901046753, Running Accuracy: 0.9627016129032258, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 123/313, Loss: 1.5567208528518677, Running Accuracy: 0.9625400641025641, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 124/313, Loss: 1.4809283018112183, Running Accuracy: 0.9627786624203821, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 125/313, Loss: 1.5005486011505127, Running Accuracy: 0.9630142405063291, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 126/313, Loss: 1.5513949394226074, Running Accuracy: 0.9626572327044025, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 127/313, Loss: 1.5394890308380127, Running Accuracy: 0.9625, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 128/313, Loss: 1.4965282678604126, Running Accuracy: 0.9625388198757764, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 129/313, Loss: 1.5448625087738037, Running Accuracy: 0.9621913580246914, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 130/313, Loss: 1.5229038000106812, Running Accuracy: 0.9622315950920245, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 131/313, Loss: 1.4887429475784302, Running Accuracy: 0.9622713414634146, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 132/313, Loss: 1.4813209772109985, Running Accuracy: 0.9625, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 133/313, Loss: 1.6115535497665405, Running Accuracy: 0.9619728915662651, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 134/313, Loss: 1.5374515056610107, Running Accuracy: 0.9618263473053892, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 135/313, Loss: 1.4888219833374023, Running Accuracy: 0.9620535714285714, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 136/313, Loss: 1.493565559387207, Running Accuracy: 0.9620931952662722, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 137/313, Loss: 1.5029252767562866, Running Accuracy: 0.9621323529411765, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 138/313, Loss: 1.52084481716156, Running Accuracy: 0.962171052631579, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 139/313, Loss: 1.502919316291809, Running Accuracy: 0.9622093023255814, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 140/313, Loss: 1.6375463008880615, Running Accuracy: 0.9613439306358381, Current Accuracy 0.8125\n",
      "Epoch: 1, Batch: 141/313, Loss: 1.4708107709884644, Running Accuracy: 0.961566091954023, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 142/313, Loss: 1.55905282497406, Running Accuracy: 0.9614285714285714, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 143/313, Loss: 1.5001862049102783, Running Accuracy: 0.9614701704545454, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 144/313, Loss: 1.506866455078125, Running Accuracy: 0.9615112994350282, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 145/313, Loss: 1.5370712280273438, Running Accuracy: 0.9615519662921348, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 146/313, Loss: 1.4820876121520996, Running Accuracy: 0.9617667597765364, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 147/313, Loss: 1.5722006559371948, Running Accuracy: 0.9616319444444444, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 148/313, Loss: 1.4999308586120605, Running Accuracy: 0.961671270718232, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 149/313, Loss: 1.5474491119384766, Running Accuracy: 0.9615384615384616, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 150/313, Loss: 1.5413625240325928, Running Accuracy: 0.9614071038251366, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 151/313, Loss: 1.5180449485778809, Running Accuracy: 0.9612771739130435, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 152/313, Loss: 1.5554404258728027, Running Accuracy: 0.9611486486486487, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 153/313, Loss: 1.5521074533462524, Running Accuracy: 0.9610215053763441, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 154/313, Loss: 1.4870553016662598, Running Accuracy: 0.9612299465240641, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 155/313, Loss: 1.5069297552108765, Running Accuracy: 0.9612699468085106, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 156/313, Loss: 1.5174193382263184, Running Accuracy: 0.9613095238095238, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 157/313, Loss: 1.4748221635818481, Running Accuracy: 0.9615131578947368, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 158/313, Loss: 1.4705392122268677, Running Accuracy: 0.9617146596858639, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 159/313, Loss: 1.5136516094207764, Running Accuracy: 0.9619140625, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 160/313, Loss: 1.4712166786193848, Running Accuracy: 0.9621113989637305, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 161/313, Loss: 1.5771536827087402, Running Accuracy: 0.9618234536082474, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 162/313, Loss: 1.505392074584961, Running Accuracy: 0.9618589743589744, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 163/313, Loss: 1.494646668434143, Running Accuracy: 0.9618941326530612, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 164/313, Loss: 1.524356722831726, Running Accuracy: 0.9619289340101523, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 165/313, Loss: 1.5507094860076904, Running Accuracy: 0.9618055555555556, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 166/313, Loss: 1.5149954557418823, Running Accuracy: 0.9618404522613065, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 167/313, Loss: 1.4672880172729492, Running Accuracy: 0.96203125, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 168/313, Loss: 1.5148435831069946, Running Accuracy: 0.9620646766169154, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 169/313, Loss: 1.520345687866211, Running Accuracy: 0.9619430693069307, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 170/313, Loss: 1.5239931344985962, Running Accuracy: 0.9618226600985221, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 171/313, Loss: 1.538940191268921, Running Accuracy: 0.9615502450980392, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 172/313, Loss: 1.5117357969284058, Running Accuracy: 0.9615853658536585, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 173/313, Loss: 1.5503897666931152, Running Accuracy: 0.9614684466019418, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 174/313, Loss: 1.501429796218872, Running Accuracy: 0.9615036231884058, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 175/313, Loss: 1.4922274351119995, Running Accuracy: 0.9616887019230769, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 176/313, Loss: 1.544352412223816, Running Accuracy: 0.961572966507177, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 177/313, Loss: 1.529016137123108, Running Accuracy: 0.9614583333333333, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 178/313, Loss: 1.49921452999115, Running Accuracy: 0.9614928909952607, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 179/313, Loss: 1.5215691328048706, Running Accuracy: 0.9613797169811321, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 180/313, Loss: 1.505319595336914, Running Accuracy: 0.9614143192488263, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 181/313, Loss: 1.5435703992843628, Running Accuracy: 0.961302570093458, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 182/313, Loss: 1.5140347480773926, Running Accuracy: 0.9611918604651163, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 183/313, Loss: 1.5172626972198486, Running Accuracy: 0.9610821759259259, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 184/313, Loss: 1.5577114820480347, Running Accuracy: 0.9609735023041475, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 185/313, Loss: 1.4934864044189453, Running Accuracy: 0.9610091743119266, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 186/313, Loss: 1.5197601318359375, Running Accuracy: 0.9610445205479452, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 187/313, Loss: 1.5221017599105835, Running Accuracy: 0.9609375, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 188/313, Loss: 1.5136922597885132, Running Accuracy: 0.9609728506787331, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 189/313, Loss: 1.5675218105316162, Running Accuracy: 0.9607263513513513, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 190/313, Loss: 1.4975132942199707, Running Accuracy: 0.9607623318385651, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 191/313, Loss: 1.4671705961227417, Running Accuracy: 0.9609375, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 192/313, Loss: 1.475748896598816, Running Accuracy: 0.9611111111111111, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 193/313, Loss: 1.5231523513793945, Running Accuracy: 0.9610066371681416, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 194/313, Loss: 1.512099027633667, Running Accuracy: 0.9611784140969163, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 195/313, Loss: 1.5182504653930664, Running Accuracy: 0.9610745614035088, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 196/313, Loss: 1.5003496408462524, Running Accuracy: 0.96110807860262, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 197/313, Loss: 1.4969459772109985, Running Accuracy: 0.9611413043478261, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 198/313, Loss: 1.530608892440796, Running Accuracy: 0.961038961038961, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 199/313, Loss: 1.513877511024475, Running Accuracy: 0.9610721982758621, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 200/313, Loss: 1.4728342294692993, Running Accuracy: 0.9612392703862661, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 201/313, Loss: 1.4853061437606812, Running Accuracy: 0.9614049145299145, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 202/313, Loss: 1.583132266998291, Running Accuracy: 0.9611702127659575, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 203/313, Loss: 1.5231239795684814, Running Accuracy: 0.9612023305084746, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 204/313, Loss: 1.4860907793045044, Running Accuracy: 0.9612341772151899, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 205/313, Loss: 1.5004249811172485, Running Accuracy: 0.961265756302521, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 206/313, Loss: 1.5119603872299194, Running Accuracy: 0.9614278242677824, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 207/313, Loss: 1.508463978767395, Running Accuracy: 0.9614583333333333, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 208/313, Loss: 1.5219762325286865, Running Accuracy: 0.9613589211618258, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 209/313, Loss: 1.505232572555542, Running Accuracy: 0.9613894628099173, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 210/313, Loss: 1.5319472551345825, Running Accuracy: 0.9612911522633745, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 211/313, Loss: 1.5328352451324463, Running Accuracy: 0.9613217213114754, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 212/313, Loss: 1.5165557861328125, Running Accuracy: 0.9613520408163265, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 213/313, Loss: 1.5410668849945068, Running Accuracy: 0.961255081300813, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 214/313, Loss: 1.4712700843811035, Running Accuracy: 0.961411943319838, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 215/313, Loss: 1.5318608283996582, Running Accuracy: 0.9614415322580645, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 216/313, Loss: 1.5037485361099243, Running Accuracy: 0.9614708835341366, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 217/313, Loss: 1.4647895097732544, Running Accuracy: 0.961625, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 218/313, Loss: 1.5127817392349243, Running Accuracy: 0.9616533864541833, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 219/313, Loss: 1.4659978151321411, Running Accuracy: 0.9618055555555556, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 220/313, Loss: 1.4921460151672363, Running Accuracy: 0.9618330039525692, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 221/313, Loss: 1.5242106914520264, Running Accuracy: 0.9618602362204725, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 222/313, Loss: 1.5256612300872803, Running Accuracy: 0.9618872549019608, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 223/313, Loss: 1.520168662071228, Running Accuracy: 0.9617919921875, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 224/313, Loss: 1.549211859703064, Running Accuracy: 0.9616974708171206, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 225/313, Loss: 1.4938298463821411, Running Accuracy: 0.9618459302325582, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 226/313, Loss: 1.4974581003189087, Running Accuracy: 0.9618725868725869, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 227/313, Loss: 1.4864178895950317, Running Accuracy: 0.9618990384615385, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 228/313, Loss: 1.506116271018982, Running Accuracy: 0.9619252873563219, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 229/313, Loss: 1.4896713495254517, Running Accuracy: 0.9619513358778626, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 230/313, Loss: 1.518937349319458, Running Accuracy: 0.9619771863117871, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 231/313, Loss: 1.5145710706710815, Running Accuracy: 0.9620028409090909, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 232/313, Loss: 1.557685136795044, Running Accuracy: 0.9617924528301887, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 233/313, Loss: 1.514955759048462, Running Accuracy: 0.9618186090225563, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 234/313, Loss: 1.4999897480010986, Running Accuracy: 0.9619616104868914, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 235/313, Loss: 1.5889880657196045, Running Accuracy: 0.9616371268656716, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 236/313, Loss: 1.478256344795227, Running Accuracy: 0.9617797397769516, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 237/313, Loss: 1.5466091632843018, Running Accuracy: 0.961574074074074, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 238/313, Loss: 1.5069109201431274, Running Accuracy: 0.9616005535055351, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 239/313, Loss: 1.534224033355713, Running Accuracy: 0.9615119485294118, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 240/313, Loss: 1.4741274118423462, Running Accuracy: 0.9616529304029304, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 241/313, Loss: 1.547536015510559, Running Accuracy: 0.9615647810218978, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 242/313, Loss: 1.5432687997817993, Running Accuracy: 0.961590909090909, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 243/313, Loss: 1.509164810180664, Running Accuracy: 0.9616168478260869, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 244/313, Loss: 1.4696377515792847, Running Accuracy: 0.9617554151624549, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 245/313, Loss: 1.5021849870681763, Running Accuracy: 0.9616681654676259, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 246/313, Loss: 1.4703516960144043, Running Accuracy: 0.9618055555555556, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 247/313, Loss: 1.4973878860473633, Running Accuracy: 0.9619419642857143, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 248/313, Loss: 1.4856399297714233, Running Accuracy: 0.9620774021352313, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 249/313, Loss: 1.5211551189422607, Running Accuracy: 0.9622118794326241, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 250/313, Loss: 1.4938558340072632, Running Accuracy: 0.962345406360424, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 251/313, Loss: 1.5010870695114136, Running Accuracy: 0.9623679577464789, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 252/313, Loss: 1.5401067733764648, Running Accuracy: 0.9623903508771929, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 253/313, Loss: 1.5616480112075806, Running Accuracy: 0.9621940559440559, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 254/313, Loss: 1.5013415813446045, Running Accuracy: 0.9622168989547039, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 255/313, Loss: 1.5286157131195068, Running Accuracy: 0.9622395833333334, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 256/313, Loss: 1.5721694231033325, Running Accuracy: 0.9619377162629758, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 257/313, Loss: 1.5451085567474365, Running Accuracy: 0.9617456896551724, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 258/313, Loss: 1.467172622680664, Running Accuracy: 0.961877147766323, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 259/313, Loss: 1.5364277362823486, Running Accuracy: 0.9619006849315068, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 260/313, Loss: 1.500847339630127, Running Accuracy: 0.9619240614334471, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 261/313, Loss: 1.4873802661895752, Running Accuracy: 0.9620535714285714, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 262/313, Loss: 1.5166172981262207, Running Accuracy: 0.9620762711864407, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 263/313, Loss: 1.505831003189087, Running Accuracy: 0.9620988175675675, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 264/313, Loss: 1.5543581247329712, Running Accuracy: 0.9620159932659933, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 265/313, Loss: 1.4735548496246338, Running Accuracy: 0.962143456375839, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 266/313, Loss: 1.4776235818862915, Running Accuracy: 0.9622700668896321, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 267/313, Loss: 1.5921592712402344, Running Accuracy: 0.9619791666666667, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 268/313, Loss: 1.500352382659912, Running Accuracy: 0.9620016611295681, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 269/313, Loss: 1.5913565158843994, Running Accuracy: 0.9617135761589404, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 270/313, Loss: 1.5770235061645508, Running Accuracy: 0.9615305280528053, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 271/313, Loss: 1.4973348379135132, Running Accuracy: 0.961657072368421, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 272/313, Loss: 1.554700255393982, Running Accuracy: 0.9614754098360656, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 273/313, Loss: 1.4790462255477905, Running Accuracy: 0.9616013071895425, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 274/313, Loss: 1.4705721139907837, Running Accuracy: 0.9617263843648208, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 275/313, Loss: 1.5232206583023071, Running Accuracy: 0.9617491883116883, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 276/313, Loss: 1.4791194200515747, Running Accuracy: 0.9618729773462783, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 277/313, Loss: 1.4698036909103394, Running Accuracy: 0.9619959677419355, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 278/313, Loss: 1.4876573085784912, Running Accuracy: 0.9620176848874598, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 279/313, Loss: 1.5146303176879883, Running Accuracy: 0.9620392628205128, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 280/313, Loss: 1.5333446264266968, Running Accuracy: 0.9619608626198083, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 281/313, Loss: 1.4813129901885986, Running Accuracy: 0.9620820063694268, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 282/313, Loss: 1.4635710716247559, Running Accuracy: 0.962202380952381, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 283/313, Loss: 1.5545322895050049, Running Accuracy: 0.9620253164556962, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 284/313, Loss: 1.4997485876083374, Running Accuracy: 0.9620465299684543, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 285/313, Loss: 1.5181214809417725, Running Accuracy: 0.9619693396226415, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 286/313, Loss: 1.5347836017608643, Running Accuracy: 0.9618926332288401, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 287/313, Loss: 1.5708599090576172, Running Accuracy: 0.96171875, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 288/313, Loss: 1.530582070350647, Running Accuracy: 0.9616433021806854, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 289/313, Loss: 1.551306962966919, Running Accuracy: 0.9615683229813664, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 290/313, Loss: 1.4903985261917114, Running Accuracy: 0.9615905572755418, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 291/313, Loss: 1.557565689086914, Running Accuracy: 0.9614197530864198, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 292/313, Loss: 1.5240832567214966, Running Accuracy: 0.9613461538461539, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 293/313, Loss: 1.4831880331039429, Running Accuracy: 0.9614647239263804, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 294/313, Loss: 1.5333750247955322, Running Accuracy: 0.961487003058104, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 295/313, Loss: 1.4668089151382446, Running Accuracy: 0.9616044207317073, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 296/313, Loss: 1.5733906030654907, Running Accuracy: 0.9613411854103343, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 297/313, Loss: 1.4939360618591309, Running Accuracy: 0.9613636363636363, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 298/313, Loss: 1.4742470979690552, Running Accuracy: 0.9614803625377644, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 299/313, Loss: 1.5680842399597168, Running Accuracy: 0.9613140060240963, Current Accuracy 0.90625\n",
      "Epoch: 1, Batch: 300/313, Loss: 1.5112173557281494, Running Accuracy: 0.9613363363363363, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 301/313, Loss: 1.4929413795471191, Running Accuracy: 0.9613585329341318, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 302/313, Loss: 1.52762770652771, Running Accuracy: 0.9613805970149254, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 303/313, Loss: 1.5152822732925415, Running Accuracy: 0.9614025297619048, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 304/313, Loss: 1.5022406578063965, Running Accuracy: 0.9614243323442137, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 305/313, Loss: 1.473809838294983, Running Accuracy: 0.9615384615384616, Current Accuracy 1.0\n",
      "Epoch: 1, Batch: 306/313, Loss: 1.6114002466201782, Running Accuracy: 0.9612831858407079, Current Accuracy 0.875\n",
      "Epoch: 1, Batch: 307/313, Loss: 1.5052788257598877, Running Accuracy: 0.9613051470588235, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 308/313, Loss: 1.492377758026123, Running Accuracy: 0.9613269794721407, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 309/313, Loss: 1.499624252319336, Running Accuracy: 0.9613486842105263, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 310/313, Loss: 1.5122597217559814, Running Accuracy: 0.9613702623906706, Current Accuracy 0.96875\n",
      "Epoch: 1, Batch: 311/313, Loss: 1.5215351581573486, Running Accuracy: 0.9613008720930233, Current Accuracy 0.9375\n",
      "Epoch: 1, Batch: 312/313, Loss: 1.5135003328323364, Running Accuracy: 0.961231884057971, Current Accuracy 0.9375\n",
      "Epoch: 1, Test Accuracy: 1.0\n",
      "Epoch: 1, Test Accuracy: 0.984375\n",
      "Epoch: 1, Test Accuracy: 0.9791666666666666\n",
      "Epoch: 1, Test Accuracy: 0.984375\n",
      "Epoch: 1, Test Accuracy: 0.9875\n",
      "Epoch: 1, Test Accuracy: 0.9791666666666666\n",
      "Epoch: 1, Test Accuracy: 0.9821428571428571\n",
      "Epoch: 1, Test Accuracy: 0.9765625\n",
      "Epoch: 1, Test Accuracy: 0.9652777777777778\n",
      "Epoch: 1, Test Accuracy: 0.9625\n",
      "Epoch: 1, Test Accuracy: 0.9630681818181818\n",
      "Epoch: 1, Test Accuracy: 0.9635416666666666\n",
      "Epoch: 1, Test Accuracy: 0.9639423076923077\n",
      "Epoch: 1, Test Accuracy: 0.9642857142857143\n",
      "Epoch: 1, Test Accuracy: 0.9625\n",
      "Epoch: 1, Test Accuracy: 0.962890625\n",
      "Epoch: 1, Test Accuracy: 0.9613970588235294\n",
      "Epoch: 1, Test Accuracy: 0.9618055555555556\n",
      "Epoch: 1, Test Accuracy: 0.962171052631579\n",
      "Epoch: 1, Test Accuracy: 0.9609375\n",
      "Epoch: 1, Test Accuracy: 0.9627976190476191\n",
      "Epoch: 1, Test Accuracy: 0.9630681818181818\n",
      "Epoch: 1, Test Accuracy: 0.9619565217391305\n",
      "Epoch: 1, Test Accuracy: 0.9596354166666666\n",
      "Epoch: 1, Test Accuracy: 0.96\n",
      "Epoch: 1, Test Accuracy: 0.9555288461538461\n",
      "Epoch: 1, Test Accuracy: 0.9548611111111112\n",
      "Epoch: 1, Test Accuracy: 0.9564732142857143\n",
      "Epoch: 1, Test Accuracy: 0.9568965517241379\n",
      "Epoch: 1, Test Accuracy: 0.9572916666666667\n",
      "Epoch: 1, Test Accuracy: 0.9566532258064516\n",
      "Epoch: 1, Test Accuracy: 0.9580078125\n",
      "Epoch: 2, Batch: 0/313, Loss: 1.4694750308990479, Running Accuracy: 0.959280303030303, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 1/313, Loss: 1.4794365167617798, Running Accuracy: 0.9604779411764706, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 2/313, Loss: 1.4997789859771729, Running Accuracy: 0.9607142857142857, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 3/313, Loss: 1.5248913764953613, Running Accuracy: 0.9600694444444444, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 4/313, Loss: 1.486533522605896, Running Accuracy: 0.9603040540540541, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 5/313, Loss: 1.5155538320541382, Running Accuracy: 0.959703947368421, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 6/313, Loss: 1.5413590669631958, Running Accuracy: 0.9591346153846154, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 7/313, Loss: 1.4717319011688232, Running Accuracy: 0.96015625, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 8/313, Loss: 1.4811800718307495, Running Accuracy: 0.9611280487804879, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 9/313, Loss: 1.4738444089889526, Running Accuracy: 0.9620535714285714, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 10/313, Loss: 1.4936707019805908, Running Accuracy: 0.9622093023255814, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 11/313, Loss: 1.5357691049575806, Running Accuracy: 0.9616477272727273, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 12/313, Loss: 1.5293729305267334, Running Accuracy: 0.9611111111111111, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 13/313, Loss: 1.4921345710754395, Running Accuracy: 0.9612771739130435, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 14/313, Loss: 1.4634608030319214, Running Accuracy: 0.9621010638297872, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 15/313, Loss: 1.5144217014312744, Running Accuracy: 0.962890625, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 16/313, Loss: 1.5302902460098267, Running Accuracy: 0.9623724489795918, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 17/313, Loss: 1.526761531829834, Running Accuracy: 0.961875, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 18/313, Loss: 1.4768909215927124, Running Accuracy: 0.9626225490196079, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 19/313, Loss: 1.4950147867202759, Running Accuracy: 0.9633413461538461, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 20/313, Loss: 1.4965426921844482, Running Accuracy: 0.9634433962264151, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 21/313, Loss: 1.5292824506759644, Running Accuracy: 0.9629629629629629, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 22/313, Loss: 1.4675251245498657, Running Accuracy: 0.9636363636363636, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 23/313, Loss: 1.523423194885254, Running Accuracy: 0.9631696428571429, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 24/313, Loss: 1.464888095855713, Running Accuracy: 0.9638157894736842, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 25/313, Loss: 1.4767979383468628, Running Accuracy: 0.9644396551724138, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 26/313, Loss: 1.5017873048782349, Running Accuracy: 0.9645127118644068, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 27/313, Loss: 1.522453784942627, Running Accuracy: 0.9645833333333333, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 28/313, Loss: 1.4839669466018677, Running Accuracy: 0.9651639344262295, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 29/313, Loss: 1.5057717561721802, Running Accuracy: 0.9652217741935484, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 30/313, Loss: 1.5098203420639038, Running Accuracy: 0.9652777777777778, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 31/313, Loss: 1.4652957916259766, Running Accuracy: 0.9658203125, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 32/313, Loss: 1.5074708461761475, Running Accuracy: 0.9658653846153846, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 33/313, Loss: 1.5587573051452637, Running Accuracy: 0.9644886363636364, Current Accuracy 0.875\n",
      "Epoch: 2, Batch: 34/313, Loss: 1.4635143280029297, Running Accuracy: 0.965018656716418, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 35/313, Loss: 1.5343775749206543, Running Accuracy: 0.9646139705882353, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 36/313, Loss: 1.5154262781143188, Running Accuracy: 0.9642210144927537, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 37/313, Loss: 1.5042887926101685, Running Accuracy: 0.9642857142857143, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 38/313, Loss: 1.4811832904815674, Running Accuracy: 0.9647887323943662, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 39/313, Loss: 1.5064995288848877, Running Accuracy: 0.96484375, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 40/313, Loss: 1.4926390647888184, Running Accuracy: 0.9648972602739726, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 41/313, Loss: 1.4642506837844849, Running Accuracy: 0.9653716216216216, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 42/313, Loss: 1.5174123048782349, Running Accuracy: 0.965, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 43/313, Loss: 1.4792309999465942, Running Accuracy: 0.9654605263157895, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 44/313, Loss: 1.4646735191345215, Running Accuracy: 0.9659090909090909, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 45/313, Loss: 1.5418193340301514, Running Accuracy: 0.9655448717948718, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 46/313, Loss: 1.5238265991210938, Running Accuracy: 0.9659810126582279, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 47/313, Loss: 1.517418384552002, Running Accuracy: 0.966015625, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 48/313, Loss: 1.5205086469650269, Running Accuracy: 0.9656635802469136, Current Accuracy 0.9375\n",
      "Epoch: 2, Batch: 49/313, Loss: 1.515048861503601, Running Accuracy: 0.9657012195121951, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 50/313, Loss: 1.5167850255966187, Running Accuracy: 0.9657379518072289, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 51/313, Loss: 1.5620228052139282, Running Accuracy: 0.9650297619047619, Current Accuracy 0.90625\n",
      "Epoch: 2, Batch: 52/313, Loss: 1.4783270359039307, Running Accuracy: 0.9654411764705882, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 53/313, Loss: 1.500749111175537, Running Accuracy: 0.9654796511627907, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 54/313, Loss: 1.4688372611999512, Running Accuracy: 0.9658764367816092, Current Accuracy 1.0\n",
      "Epoch: 2, Batch: 55/313, Loss: 1.4961144924163818, Running Accuracy: 0.9659090909090909, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 56/313, Loss: 1.5001440048217773, Running Accuracy: 0.9659410112359551, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 57/313, Loss: 1.5378928184509277, Running Accuracy: 0.9659722222222222, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 58/313, Loss: 1.5206570625305176, Running Accuracy: 0.9660027472527473, Current Accuracy 0.96875\n",
      "Epoch: 2, Batch: 59/313, Loss: 1.5169119834899902, Running Accuracy: 0.9660326086956522, Current Accuracy 0.96875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# calculate accuracy\u001b[39;00m\n\u001b[1;32m     27\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, pred\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     29\u001b[0m acc_list\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     30\u001b[0m running_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(acc_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(acc_list)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use corss entropy loss\n",
    "# import accuracy and f1 from sklearn\n",
    "model = ptuned_VIT(classes)\n",
    "model = model.to(device)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# use adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "acc_list = []\n",
    "n_epochs = 10\n",
    "fin_acc = []\n",
    "for epoch in range(n_epochs):\n",
    "    batch_no = 0\n",
    "    for sample in train_loader:\n",
    "        image = sample[0]\n",
    "        label = sample[1]\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        # iamge shape is a, 1, b, c, d make it a, b, c, d\n",
    "        image = image.squeeze(1)\n",
    "        output = model(image)\n",
    "        loss = loss_func(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # calculate accuracy\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        acc = accuracy_score(label.cpu(), pred.cpu())\n",
    "        acc_list.append(acc)\n",
    "        running_acc = sum(acc_list)/len(acc_list)\n",
    "        fin_acc.append(acc)\n",
    "        print(f\"Epoch: {epoch}, Batch: {batch_no}/313, Loss: {loss.item()}, Running Accuracy: {running_acc}, Current Accuracy {acc}\")\n",
    "        batch_no += 1\n",
    "    # write testing loop using test_loader\n",
    "    acc_list = []\n",
    "    for sample in test_loader:\n",
    "        image = sample[0]\n",
    "        label = sample[1]\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        image = image.squeeze(1)\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        acc = accuracy_score(label.cpu(), pred.cpu())\n",
    "        acc_list.append(acc)\n",
    "        running_acc = sum(acc_list)/len(acc_list)\n",
    "        print(f\"Epoch: {epoch}, Test Accuracy: {running_acc}\")\n",
    "    torch.save(model, \"pt_vit_cifar_model.pt\")\n",
    "    torch.save(fin_acc, \"pt_vit_cifar_acc.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"pt_vit_cifar_model.pt\")\n",
    "torch.save(fin_acc, \"pt_vit_cifar_acc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 0.0625, 0.21875, 0.125, 0.25, 0.1875, 0.28125, 0.34375, 0.46875, 0.46875, 0.53125, 0.59375, 0.375, 0.46875, 0.59375, 0.34375, 0.5, 0.59375, 0.5625, 0.59375, 0.5625, 0.6875, 0.6875, 0.65625, 0.65625, 0.65625, 0.78125, 0.78125, 0.78125, 0.78125, 0.90625, 0.71875, 0.78125, 0.84375, 0.8125, 0.90625, 0.96875, 0.90625, 0.96875, 0.96875, 0.84375, 0.8125, 0.875, 0.875, 0.84375, 0.78125, 0.8125, 0.9375, 0.875, 0.9375, 0.9375, 0.84375, 0.875, 0.78125, 0.9375, 0.90625, 0.96875, 0.9375, 0.90625, 0.9375, 0.84375, 0.875, 0.875, 0.96875, 0.9375, 0.96875, 0.84375, 0.875, 0.96875, 0.90625, 0.90625, 0.90625, 0.96875, 0.90625, 0.90625, 0.96875, 0.96875, 0.8125, 0.90625, 0.9375, 0.9375, 0.9375, 0.9375, 1.0, 0.90625, 0.875, 0.90625, 0.96875, 1.0, 0.9375, 0.875, 0.90625, 0.96875, 0.875, 0.8125, 0.9375, 0.96875, 0.9375, 0.9375, 0.9375, 0.9375, 0.875, 0.9375, 0.90625, 0.9375, 0.96875, 0.9375, 0.96875, 0.96875, 0.90625, 0.90625, 0.96875, 0.875, 0.9375, 0.96875, 0.9375, 0.9375, 1.0, 1.0, 1.0, 0.84375, 1.0, 0.90625, 0.90625, 0.96875, 0.90625, 0.96875, 0.90625, 1.0, 0.9375, 0.9375, 0.96875, 0.9375, 0.9375, 0.9375, 0.90625, 1.0, 0.9375, 0.9375, 0.96875, 0.875, 0.96875, 0.90625, 0.90625, 0.96875, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 0.9375, 0.96875, 0.9375, 1.0, 1.0, 0.9375, 0.9375, 0.9375, 0.9375, 0.90625, 0.9375, 1.0, 0.875, 0.96875, 1.0, 0.875, 0.875, 0.96875, 0.9375, 0.96875, 0.90625, 0.96875, 0.9375, 0.96875, 0.90625, 0.96875, 0.9375, 0.96875, 0.90625, 0.9375, 0.96875, 0.9375, 1.0, 0.90625, 0.9375, 0.8125, 1.0, 0.9375, 1.0, 0.9375, 0.96875, 0.9375, 0.9375, 1.0, 0.9375, 0.9375, 0.96875, 1.0, 0.96875, 1.0, 0.875, 1.0, 0.96875, 0.90625, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 0.90625, 0.96875, 0.9375, 0.96875, 0.96875, 1.0, 0.9375, 0.96875, 0.96875, 0.90625, 1.0, 0.90625, 0.875, 1.0, 0.90625, 0.96875, 0.875, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.9375, 0.90625, 0.96875, 1.0, 0.90625, 0.875, 0.90625, 0.9375, 1.0, 0.96875, 0.90625, 0.96875, 0.90625, 0.90625, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 0.9375, 1.0, 0.9375, 0.875, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.9375, 0.96875, 0.9375, 1.0, 0.875, 0.875, 0.90625, 1.0, 0.90625, 0.9375, 1.0, 0.9375, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 0.9375, 0.96875, 0.875, 0.9375, 0.96875, 0.96875, 0.9375, 0.9375, 0.9375, 0.9375, 0.96875, 0.96875, 0.9375, 0.90625, 0.90625, 0.9375, 0.96875, 0.9375, 1.0, 0.96875, 0.90625, 0.96875, 0.96875, 0.96875, 1.0, 0.9375, 0.9375, 1.0, 1.0, 1.0, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 0.9375, 0.9375, 0.9375, 0.96875, 1.0, 0.9375, 0.875, 0.96875, 0.96875, 0.9375, 0.96875, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.90625, 0.96875, 0.96875, 0.9375, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 0.9375, 0.9375, 0.9375, 1.0, 0.96875, 0.875, 0.96875, 0.96875, 0.9375, 0.96875, 0.9375, 0.96875, 0.9375, 1.0, 1.0, 0.9375, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.9375, 1.0, 0.96875, 0.9375, 0.96875, 0.96875, 0.96875, 0.875, 0.90625, 0.96875, 0.96875, 0.90625, 0.96875, 0.90625, 1.0, 0.96875, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.96875, 0.96875, 0.9375, 0.9375, 0.96875, 1.0, 1.0, 0.9375, 0.96875, 1.0, 0.96875, 0.90625, 0.96875, 0.90625, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 0.96875, 0.9375, 0.96875, 1.0, 0.96875, 1.0, 1.0, 0.9375, 0.96875, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 0.96875, 0.875, 0.9375, 1.0, 0.9375, 1.0, 1.0, 0.90625, 0.9375, 0.96875, 0.90625, 0.96875, 0.96875, 1.0, 0.875, 0.9375, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.8125, 1.0, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 0.9375, 0.96875, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.96875, 0.96875, 0.96875, 0.9375, 0.96875, 1.0, 0.96875, 0.9375, 0.9375, 0.90625, 0.96875, 0.9375, 0.96875, 1.0, 0.9375, 0.9375, 0.96875, 0.9375, 0.96875, 0.9375, 0.9375, 0.9375, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 0.90625, 0.96875, 1.0, 1.0, 0.9375, 1.0, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 1.0, 1.0, 0.90625, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 0.9375, 0.96875, 0.9375, 0.96875, 0.96875, 0.9375, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 0.9375, 0.9375, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.90625, 0.96875, 1.0, 0.875, 1.0, 0.90625, 0.96875, 0.9375, 1.0, 0.9375, 0.96875, 0.96875, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.96875, 0.90625, 0.96875, 0.96875, 0.875, 0.90625, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 0.875, 0.96875, 0.875, 0.90625, 1.0, 0.90625, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 0.90625, 0.96875, 0.9375, 0.9375, 0.90625, 0.9375, 0.9375, 0.96875, 0.90625, 0.9375, 1.0, 0.96875, 1.0, 0.875, 0.96875, 1.0, 0.90625, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 1.0, 0.875, 0.96875, 0.96875, 0.96875, 0.96875, 0.9375, 0.9375, 1.0, 1.0, 0.96875, 0.9375, 0.96875, 0.9375, 0.9375, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.9375, 0.96875, 1.0, 1.0, 0.9375, 0.9375, 1.0, 1.0, 0.96875, 0.9375, 1.0, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.875, 1.0, 0.9375, 0.9375, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 0.9375, 1.0, 1.0, 0.9375, 1.0, 0.96875, 0.9375, 0.96875, 0.96875, 0.90625, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_mod = torch.load(\"pt_vit_cifar_model.pt\")\n",
    "new_list = torch.load(\"pt_vit_cifar_acc.pt\")\n",
    "print(new_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
