{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/csai_a4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'csai_a4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# move cwd to csai_a4\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma4_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m stim_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSI01_stim_lists.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# opena nd load the file\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'csai_a4'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "a4_path = \"csai_a4\"\n",
    "# print cwd\n",
    "print(os.getcwd())\n",
    "# move cwd to csai_a4\n",
    "os.chdir(a4_path)\n",
    "stim_path = \"CSI01_stim_lists.txt\"\n",
    "# opena nd load the file\n",
    "with open(stim_path, 'r') as f:\n",
    "    stim_list = f.read().splitlines()\n",
    "file_set = []\n",
    "indices_list = []\n",
    "for i in range(len(stim_list)):\n",
    "    file = stim_list[i]\n",
    "    # first 3 chars of the file name is not rep, add to the set\n",
    "    if file[:3] != \"rep\":\n",
    "        indices_list.append(i)\n",
    "        file_set.append(file)\n",
    "print(len(indices_list))\n",
    "fin_images = []\n",
    "for i in indices_list:\n",
    "    fin_images.append(stim_list[i])\n",
    "for file in fin_images:\n",
    "    # if file has first 3 chars as rep, print file name\n",
    "    if file[:3] == \"rep\":\n",
    "        print(file)\n",
    "print(len(file_set))\n",
    "print(stim_list[0:20])\n",
    "# for coco files: first 4 chars are COCO\n",
    "# for imagenet files: first 2 chars are n0 or n1\n",
    "# rest are Scene files\n",
    "coco_files = []\n",
    "imagenet_files = []\n",
    "scene_files = []\n",
    "for i in range(len(stim_list)):\n",
    "    if stim_list[i][0:4] == \"COCO\":\n",
    "        #print(stim_list[i][0:4], stim_list[i], \"coco\")\n",
    "        coco_files.append(stim_list[i])\n",
    "    elif stim_list[i][0:2] == \"n0\" or stim_list[i][0:2] == \"n1\":\n",
    "        #print(stim_list[i][0:2], stim_list[i], \"imagenet\")\n",
    "        imagenet_files.append(stim_list[i])\n",
    "    else:\n",
    "        #print(stim_list[i], \"scene\")\n",
    "        scene_files.append(stim_list[i] )\n",
    "print(coco_files[0], scene_files[0], imagenet_files[0])\n",
    "coco_dir = \"presented_stimuli/COCO/\"\n",
    "imagenet_dir = \"presented_stimuli/ImageNet/\"\n",
    "scene_dir = \"presented_stimuli/Scene/\"\n",
    "# check if these directories exist\n",
    "if not os.path.exists(coco_dir):\n",
    "    print(\"COCO directory does not exist\")\n",
    "if not os.path.exists(imagenet_dir):\n",
    "    print(\"Imagenet directory does not exist\")\n",
    "if not os.path.exists(scene_dir):\n",
    "    print(\"Scene directory does not exist\")\n",
    "root_path = \"CSI1/h5/\"\n",
    "# load all the files in the root path into list\n",
    "list_files = []\n",
    "for root, dirs, files in os.walk(root_path):\n",
    "    for file in files:\n",
    "        list_files.append(file)\n",
    "\n",
    "print(list_files)\n",
    "# load file CSI1/h5/CSI1_ROIs_TR1.h5\n",
    "!pip install h5py\n",
    "import h5py\n",
    "path = \"CSI1/h5/CSI1_ROIs_TR34.h5\"\n",
    "f = h5py.File(path, 'r')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading FMRI Data and Removing Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHEarlyVis (5254, 210)\n",
      "LHLOC (5254, 152)\n",
      "LHOPA (5254, 101)\n",
      "LHPPA (5254, 131)\n",
      "LHRSC (5254, 86)\n",
      "RHEarlyVis (5254, 285)\n",
      "RHLOC (5254, 190)\n",
      "RHOPA (5254, 187)\n",
      "RHPPA (5254, 200)\n",
      "RHRSC (5254, 143)\n",
      "(5254, 342) (5254, 288) (5254, 331) (5254, 229) (5254, 495)\n",
      "4916 4916\n",
      "(4916, 342) (4916, 288) (4916, 331) (4916, 229) (4916, 495)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "keys = list(f.keys())\n",
    "for key in keys:\n",
    "    print(key, f[key].shape)\n",
    "# concat LHLOC and RHLOC along dim 1\n",
    "lhloc = f[\"LHLOC\"]\n",
    "rhloc = f[\"RHLOC\"]\n",
    "lhloc.shape, rhloc.shape\n",
    "loc = np.concatenate([lhloc, rhloc], axis=1)\n",
    "loc.shape\n",
    "# same for OPA \n",
    "lhopa = f[\"LHOPA\"]\n",
    "rhopa = f[\"RHOPA\"]\n",
    "opa = np.concatenate([lhopa, rhopa], axis=1)\n",
    "# same for PPA \n",
    "lhppa = f[\"LHPPA\"]\n",
    "rhppa = f[\"RHPPA\"]\n",
    "ppa = np.concatenate([lhppa, rhppa], axis=1)\n",
    "# same for RSC\n",
    "lhrsc = f[\"LHRSC\"]\n",
    "rhrsc = f[\"RHRSC\"]\n",
    "rsc = np.concatenate([lhrsc, rhrsc], axis=1)\n",
    "# same for EarlyVis\n",
    "lhearlyvis = f[\"LHEarlyVis\"]\n",
    "rhearlyvis = f[\"RHEarlyVis\"]\n",
    "earlyvis = np.concatenate([lhearlyvis, rhearlyvis], axis=1)\n",
    "print(loc.shape, opa.shape, ppa.shape, rsc.shape, earlyvis.shape)\n",
    "print(len(indices_list), len(fin_images))\n",
    "# in array loc, include only the indices in indices_list\n",
    "loc_final = []\n",
    "for i in indices_list:\n",
    "    loc_final.append(loc[i])\n",
    "loc_final = np.array(loc_final)\n",
    "# do the same for opa, ppa, rsc, earlyvis\n",
    "opa_final = []\n",
    "for i in indices_list:\n",
    "    opa_final.append(opa[i])\n",
    "opa_final = np.array(opa_final)\n",
    "ppa_final = []\n",
    "for i in indices_list:\n",
    "    ppa_final.append(ppa[i])\n",
    "ppa_final = np.array(ppa_final)\n",
    "rsc_final = []\n",
    "for i in indices_list:\n",
    "    rsc_final.append(rsc[i])\n",
    "rsc_final = np.array(rsc_final)\n",
    "earlyvis_final = []\n",
    "for i in indices_list:\n",
    "    earlyvis_final.append(earlyvis[i])\n",
    "earlyvis_final = np.array(earlyvis_final)\n",
    "print(loc_final.shape, opa_final.shape, ppa_final.shape, rsc_final.shape, earlyvis_final.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulus Loading and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "filename = fin_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel, AutoImageProcessor\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "class ptuned_VIT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ptuned_VIT, self).__init__()\n",
    "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.embeddings = nn.Embedding(3, 768)\n",
    "        self.classification_layer = nn.Linear(768, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.trainable = [self.embeddings, self.classification_layer]\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        tens = [0,1,2]\n",
    "        tens = torch.tensor(tens)\n",
    "        x1 = self.embeddings(tens)\n",
    "        x2 = self.model.embeddings(x)\n",
    "        # change x1 shape from x, 768 to 1, x, 768\n",
    "        x1 = x1.unsqueeze(0)\n",
    "        # concat x1 and x2 along the first dimension\n",
    "        # x1 is 1, 3, 768, make is x2.shape[0], 3, 768\n",
    "        x1 = x1.expand(x2.shape[0], -1, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = self.model.encoder(x)\n",
    "        x = x['last_hidden_state']\n",
    "        x1 = x\n",
    "        x = self.model.pooler(x)\n",
    "        x = self.classification_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x, x1\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "mnist_pt = torch.load(\"pt_vit_mnist_model.pt\")\n",
    "cifar_pt = torch.load(\"pt_vit_cifar_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4916/4916 [08:53<00:00,  9.21it/s]\n"
     ]
    }
   ],
   "source": [
    "test_file = open(\"test.txt\", \"w\")\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "def get_feat(files):\n",
    "    mnist_feats = []\n",
    "    cifar_feats = []\n",
    "    coco_dir = \"presented_stimuli/COCO/\"\n",
    "    imagenet_dir = \"presented_stimuli/ImageNet/\"\n",
    "    scene_dir = \"presented_stimuli/Scene/\"\n",
    "    new_files = []\n",
    "    for file in files:\n",
    "        if file[0:4] == \"COCO\":\n",
    "            new_path = coco_dir + file\n",
    "        elif file[0:2] == \"n0\" or file[0:2] == \"n1\":\n",
    "            new_path = imagenet_dir + file\n",
    "        else:\n",
    "            new_path = scene_dir + file\n",
    "        new_files.append(new_path)\n",
    "    mnist_pt.to(\"cpu\")\n",
    "    for filename in tqdm(new_files, total = len(new_files)):\n",
    "        img = Image.open(filename)\n",
    "        img = processor(img, return_tensors=\"pt\")\n",
    "        img = img['pixel_values']\n",
    "        img = img.to(\"cpu\")\n",
    "        logits, feats = mnist_pt(img)\n",
    "        # convert from 1, a, b to a, b\n",
    "        feats = feats.squeeze(0)\n",
    "        feats_mnist = torch.mean(feats, dim=0)\n",
    "        feats_mnist = feats_mnist.detach().numpy()\n",
    "        mnist_feats.append(feats_mnist)\n",
    "        cifar_pt.to(\"cpu\")\n",
    "        logits, feats = cifar_pt(img)\n",
    "        feats = feats.squeeze(0)\n",
    "        feats_cifar = torch.mean(feats, dim=0)\n",
    "        feats_cifar = feats_cifar.detach().numpy()\n",
    "        cifar_feats.append(feats_cifar)\n",
    "    mnist_feats = np.array(mnist_feats)\n",
    "    cifar_feats = np.array(cifar_feats)\n",
    "    return mnist_feats, cifar_feats\n",
    "\n",
    "    \n",
    "mnist_feats, cifar_feats = get_feat(fin_images)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# import R2 as well\n",
    "from sklearn.metrics import r2_score\n",
    "def cos_d(y1, y2):\n",
    "    cos_sim = np.dot(y1, y2)/(np.linalg.norm(y1)*np.linalg.norm(y2))\n",
    "    cos_d = 1 - cos_sim\n",
    "    return cos_d\n",
    "def get_2v2_accuracy(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_cnt = 0\n",
    "    pos_cnt = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(1, N):\n",
    "            yi_test = y_test[i]\n",
    "            yi_pred = y_pred[i]\n",
    "            yj_test = y_test[j]\n",
    "            yj_pred = y_pred[j]\n",
    "            score1 = cos_d(yi_test, yi_pred) + cos_d(yj_test, yj_pred)\n",
    "            score2 = cos_d(yi_test, yj_pred) + cos_d(yj_test, yi_pred)\n",
    "            if score1 < score2:\n",
    "                pos_cnt += 1\n",
    "            tot_cnt += 1\n",
    "    return pos_cnt/tot_cnt\n",
    "\n",
    "def get_avg_corr(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_corr = 0\n",
    "    for i in range(0, N):\n",
    "        corr = np.corrcoef(y_pred[i], y_test[i])[0,1]\n",
    "        tot_corr += corr\n",
    "    return tot_corr/N\n",
    "\n",
    "def decoding_module(X, y):\n",
    "    kf = KFold(n_splits=5)\n",
    "    mse = []\n",
    "    r2 = []\n",
    "    acc = []\n",
    "    acc_scores = []\n",
    "    corr_scores = []\n",
    "    for train_index, test_index in  kf.split(X) :\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = Ridge()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        get_2v2_accuracy(y_pred, y_test)\n",
    "        mse.append(mean_squared_error(y_test, y_pred))\n",
    "        r2.append(r2_score(y_test, y_pred))\n",
    "        acc_scores.append(get_2v2_accuracy(y_pred, y_test))\n",
    "        corr_scores.append(get_avg_corr(y_pred, y_test))\n",
    "    loc_dict = {}\n",
    "    loc_dict[\"MSE\"] = np.mean(mse)\n",
    "    loc_dict[\"R2\"] = np.mean(r2)\n",
    "    loc_dict[\"Accuracy\"] = np.mean(acc_scores)\n",
    "    loc_dict[\"Correlation\"] = np.mean(corr_scores)\n",
    "    return loc_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 46.3713631634123, 'R2': 0.002492346152606733, 'Accuracy': 0.7562787163037682, 'Correlation': 0.6224184502389152}\n",
      "{'MSE': 46.10152688761668, 'R2': 0.004675018122699026, 'Accuracy': 0.7495915916565418, 'Correlation': 0.6241753485593234}\n",
      "{'MSE': 46.64586474428514, 'R2': -0.00671368236643888, 'Accuracy': 0.7187971272060846, 'Correlation': 0.6183939282539741}\n",
      "{'MSE': 46.49262379839511, 'R2': -0.002334279737623587, 'Accuracy': 0.6835862199911704, 'Correlation': 0.6206345995111626}\n",
      "{'MSE': 46.473650719804375, 'R2': -0.002213545636266707, 'Accuracy': 0.7030619466233033, 'Correlation': 0.6203937039943589}\n",
      "{'MSE': 46.70773813906378, 'R2': 0.0023301087906300194, 'Accuracy': 0.7544310246858997, 'Correlation': 0.6225831668853784}\n",
      "{'MSE': 46.42708902266624, 'R2': 0.0045523580480065304, 'Accuracy': 0.7477646462297923, 'Correlation': 0.6243655380481512}\n",
      "{'MSE': 46.975741357705004, 'R2': -0.0068418197380436, 'Accuracy': 0.7166065521933406, 'Correlation': 0.6185793279790157}\n",
      "{'MSE': 46.818912669189366, 'R2': -0.0023860232153241974, 'Accuracy': 0.6823463511355656, 'Correlation': 0.6208763326076105}\n",
      "{'MSE': 46.80543080855391, 'R2': -0.002322437378973898, 'Accuracy': 0.7017334951321472, 'Correlation': 0.6205714651276635}\n"
     ]
    }
   ],
   "source": [
    "feats = [mnist_feats, cifar_feats]\n",
    "feat_names = [\"mnist\", \"cifar\"]\n",
    "rois = [loc_final, opa_final, ppa_final, rsc_final, earlyvis_final]\n",
    "roi_names = [\"LOC\", \"OPA\", \"PPA\", \"RSC\", \"EarlyVis\"]\n",
    "decoding_results = {}\n",
    "for i in range(len(feats)):\n",
    "    for j in range(len(rois)):\n",
    "        str1 = f\"Decoding {feat_names[i]} using {roi_names[j]}\"\n",
    "        loc_feat = feats[i]\n",
    "        loc_roi = rois[j]\n",
    "        x = loc_roi\n",
    "        y = loc_feat\n",
    "        loc_results = decoding_module(x,y)\n",
    "        print(loc_results)\n",
    "        decoding_results[str1] = loc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 0.00012173177579614387, 'R2': -0.20656616415260465, 'Accuracy': 0.7058057216752819, 'Correlation': 0.19876371096776418}\n",
      "{'MSE': 9.074416686662823e-05, 'R2': -0.16161847518649294, 'Accuracy': 0.7594152030884718, 'Correlation': 0.26390564970825015}\n",
      "{'MSE': 0.0003084225118174564, 'R2': -0.18651588344267048, 'Accuracy': 0.7384781920704903, 'Correlation': 0.1461920116518157}\n",
      "{'MSE': 0.0002546602404998866, 'R2': -0.16634132673139762, 'Accuracy': 0.6822310500672949, 'Correlation': 0.09629421117604631}\n",
      "{'MSE': 0.00015525178506046043, 'R2': -0.22954436483387325, 'Accuracy': 0.6396903132902682, 'Correlation': 0.20718234999451032}\n",
      "{'MSE': 0.0001217564987661722, 'R2': -0.20690190543596937, 'Accuracy': 0.7062302789061252, 'Correlation': 0.19895001853764083}\n",
      "{'MSE': 9.078429845725822e-05, 'R2': -0.16210771280006023, 'Accuracy': 0.75909401465501, 'Correlation': 0.26383820224223786}\n",
      "{'MSE': 0.00030851803168683304, 'R2': -0.1866873353299835, 'Accuracy': 0.7376128364974085, 'Correlation': 0.14599974929197007}\n",
      "{'MSE': 0.00025466407797792234, 'R2': -0.16667836174036102, 'Accuracy': 0.6817787948921004, 'Correlation': 0.09644127672470651}\n",
      "{'MSE': 0.00015517065064884274, 'R2': -0.22923474517435766, 'Accuracy': 0.6403259641211712, 'Correlation': 0.20748057027418687}\n"
     ]
    }
   ],
   "source": [
    "feats = [mnist_feats, cifar_feats]\n",
    "feat_names = [\"mnist\", \"cifar\"]\n",
    "rois = [loc_final, opa_final, ppa_final, rsc_final, earlyvis_final]\n",
    "roi_names = [\"LOC\", \"OPA\", \"PPA\", \"RSC\", \"EarlyVis\"]\n",
    "encoding_results = {}\n",
    "for i in range(len(feats)):\n",
    "    for j in range(len(rois)):\n",
    "        str1 = f\"Decoding {feat_names[i]} using {roi_names[j]}\"\n",
    "        loc_feat = feats[i]\n",
    "        loc_roi = rois[j]\n",
    "        y = loc_roi\n",
    "        x = loc_feat\n",
    "        loc_results = decoding_module(x,y)\n",
    "        print(loc_results)\n",
    "        encoding_results[str1] = loc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoding_results as encoding_ft_vis.json and indent 4\n",
    "import json\n",
    "with open(\"encoding_pt_vis.json\", \"w\") as f:\n",
    "    json.dump(encoding_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for decoding\n",
    "with open(\"decoding_pt_vis.json\", \"w\") as f:\n",
    "    json.dump(decoding_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/csai_a4\n"
     ]
    }
   ],
   "source": [
    "# print cwd\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
