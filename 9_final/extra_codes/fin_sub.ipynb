{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptuned_trans_model = \"./translation_best\"\n",
    "promptuned_qa_model = \"./qa_best\"\n",
    "promptuned_summ_model = \"./summarisation_best\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print cwd\n",
    "print(os.getcwd())\n",
    "path = \"./models/summ_pt.pt\"\n",
    "summ_test = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \" I hope to god this works\"\n",
    "inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "output = summ_test(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_test.save_pretrained(\"summ_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open promptuned model directory and load gpt2 model \n",
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config\n",
    "qa_config = AutoConfig.from_pretrained(promptuned_qa_model)\n",
    "qa_model = GPT2Model.from_pretrained(promptuned_qa_model, config=qa_config)\n",
    "qa_model.eval()\n",
    "summ_config = AutoConfig.from_pretrained(\"summ_test\")\n",
    "summ_model = GPT2Model.from_pretrained(promptuned_summ_model, config=summ_config)\n",
    "summ_model.eval()\n",
    "trans_config = AutoConfig.from_pretrained(promptuned_trans_model)\n",
    "trans_model = GPT2Model.from_pretrained(promptuned_trans_model, config=trans_config)\n",
    "trans_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 768])\n",
      "torch.Size([1, 13, 768])\n",
      "torch.Size([1, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "sent = \"I want to test if this model will add the tokens by itself\"\n",
    "input_ids = tokenizer.encode(sent, return_tensors=\"pt\")\n",
    "outputs_qa = qa_model(input_ids)\n",
    "outputs_summ = summ_model(input_ids)\n",
    "outputs_trans = trans_model(input_ids)\n",
    "print(outputs_qa['last_hidden_state'].shape)\n",
    "print(outputs_summ['last_hidden_state'].shape)\n",
    "print(outputs_trans['last_hidden_state'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "def get_representations(list_of_sents):\n",
    "    summ_reps = []\n",
    "    qa_reps = []\n",
    "    trans_reps = []\n",
    "    for sent in tqdm(list_of_sents, total = len(list_of_sents)):\n",
    "        #sent = \" \".join(sent)\n",
    "        #print(sent)\n",
    "        input_ids = tokenizer.encode(sent, return_tensors=\"pt\")\n",
    "        #print(input_ids[0].shape)\n",
    "        #print(\"ch1\")\n",
    "        outputs_qa = qa_model(input_ids)\n",
    "        #print(\"ch2\")\n",
    "        outputs_summ = summ_model(input_ids)\n",
    "        #print(\"ch3\")\n",
    "        outputs_trans = trans_model(input_ids)\n",
    "        #print(\"ch4\")\n",
    "        outputs_qa = outputs_qa['last_hidden_state']\n",
    "        outputs_summ = outputs_summ['last_hidden_state']\n",
    "        outputs_trans = outputs_trans['last_hidden_state']\n",
    "        #print(\"ch5\")\n",
    "        # get mean_pooled representations of each one\n",
    "        summ_pooled = torch.mean(outputs_summ, dim=1)\n",
    "        qa_pooled = torch.mean(outputs_qa, dim=1)\n",
    "        trans_pooled = torch.mean(outputs_trans, dim=1)\n",
    "        #print(\"ch6\")\n",
    "        summ_pooled = np.array(summ_pooled.detach().numpy())\n",
    "        qa_pooled = np.array(qa_pooled.detach().numpy())\n",
    "        trans_pooled = np.array(trans_pooled.detach().numpy())\n",
    "        # reshape from 1,x to x\n",
    "        summ_pooled = summ_pooled.reshape(-1)\n",
    "        qa_pooled = qa_pooled.reshape(-1)\n",
    "        trans_pooled = trans_pooled.reshape(-1)\n",
    "        summ_reps.append(summ_pooled)\n",
    "        qa_reps.append(qa_pooled)\n",
    "        trans_reps.append(trans_pooled)\n",
    "    return summ_reps, qa_reps, trans_reps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"I am here\"\n",
    "sent2 = \"hi in test right now\"\n",
    "sent3 = \"lets see if it works\"\n",
    "list_sents = [sent1, sent2, sent3]\n",
    "summ_reps, qa_reps, trans_reps = get_representations(list_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/advaith.malladi\n"
     ]
    }
   ],
   "source": [
    "# print cwd\n",
    "import os\n",
    "print(os.getcwd())\n",
    "working_dir = \"csai_a5\"\n",
    "# switch to working dir\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parent_dir = \"assignment5\"\n",
    "stim_file = \"stimuli.txt\"\n",
    "subj1 = \"subj1.npy\"\n",
    "subj2 = \"subj2.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stims file\n",
    "import regex as re\n",
    "with open(os.path.join(parent_dir, stim_file), \"r\") as f:\n",
    "    stims = f.readlines()\n",
    "final_stims = []\n",
    "for item in stims:\n",
    "    # remove all numbers and punctuation\n",
    "    item = re.sub(r'[^\\w\\s]','',item)\n",
    "    # remove all numbers\n",
    "    item = re.sub(r'\\d+', '', item)\n",
    "    item = item.lower().split()\n",
    "    final_stims.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 627/627 [00:33<00:00, 18.54it/s]\n"
     ]
    }
   ],
   "source": [
    "summ_reps, qa_reps, trans_reps = get_representations(final_stims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fmri data of subj1 and subj2\n",
    "import numpy as np\n",
    "subj1_data = np.load(os.path.join(parent_dir, subj1), allow_pickle=True).item()\n",
    "subj2_data = np.load(os.path.join(parent_dir, subj2), allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627, 11437) (627, 33792) (627, 17190) (627, 35120) (627, 10791) (627, 31109) (627, 15070) (627, 30594)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subj1_lang = subj1_data[\"language\"]\n",
    "subj1_vis = subj1_data[\"vision\"]\n",
    "subj1_dmn = subj1_data[\"dmn\"]\n",
    "subj1_task = subj1_data[\"task\"]\n",
    "subj2_lang = subj2_data[\"language\"]\n",
    "subj2_vis = subj2_data[\"vision\"]\n",
    "subj2_dmn = subj2_data[\"dmn\"]\n",
    "subj2_task = subj2_data[\"task\"]\n",
    "print(subj1_lang.shape, subj1_vis.shape, subj1_dmn.shape, subj1_task.shape, subj2_lang.shape, subj2_vis.shape, subj2_dmn.shape, subj2_task.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# import R2 as well\n",
    "from sklearn.metrics import r2_score\n",
    "def cos_d(y1, y2):\n",
    "    cos_sim = np.dot(y1, y2)/(np.linalg.norm(y1)*np.linalg.norm(y2))\n",
    "    cos_d = 1 - cos_sim\n",
    "    return cos_d\n",
    "def get_2v2_accuracy(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_cnt = 0\n",
    "    pos_cnt = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(1, N):\n",
    "            yi_test = y_test[i]\n",
    "            yi_pred = y_pred[i]\n",
    "            yj_test = y_test[j]\n",
    "            yj_pred = y_pred[j]\n",
    "            score1 = cos_d(yi_test, yi_pred) + cos_d(yj_test, yj_pred)\n",
    "            score2 = cos_d(yi_test, yj_pred) + cos_d(yj_test, yi_pred)\n",
    "            if score1 < score2:\n",
    "                pos_cnt += 1\n",
    "            tot_cnt += 1\n",
    "    return pos_cnt/tot_cnt\n",
    "\n",
    "def get_avg_corr(y_pred, y_test):\n",
    "    N = y_pred.shape[0]\n",
    "    tot_corr = 0\n",
    "    for i in range(0, N):\n",
    "        corr = np.corrcoef(y_pred[i], y_test[i])[0,1]\n",
    "        tot_corr += corr\n",
    "    return tot_corr/N\n",
    "\n",
    "def decoding_module(X, y):\n",
    "    kf = KFold(n_splits=5)\n",
    "    mse = []\n",
    "    r2 = []\n",
    "    acc = []\n",
    "    acc_scores = []\n",
    "    corr_scores = []\n",
    "    for train_index, test_index in  kf.split(X) :\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = Ridge()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        get_2v2_accuracy(y_pred, y_test)\n",
    "        mse.append(mean_squared_error(y_test, y_pred))\n",
    "        r2.append(r2_score(y_test, y_pred))\n",
    "        acc_scores.append(get_2v2_accuracy(y_pred, y_test))\n",
    "        corr_scores.append(get_avg_corr(y_pred, y_test))\n",
    "    loc_dict = {}\n",
    "    loc_dict[\"MSE\"] = np.mean(mse)\n",
    "    loc_dict[\"R2\"] = np.mean(r2)\n",
    "    loc_dict[\"Accuracy\"] = np.mean(acc_scores)\n",
    "    loc_dict[\"Correlation\"] = np.mean(corr_scores)\n",
    "    return loc_dict\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "subj1_decoding_scores = {}\n",
    "subj2_decoding_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(summ_reps)\n",
    "y = subj1_lang\n",
    "decoding_scores = decoding_module(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 25.365624188836136, 'R2': -0.4493539880303308, 'Accuracy': 0.6715242191500256, 'Correlation': 0.6844155329498589}\n"
     ]
    }
   ],
   "source": [
    "print(decoding_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 25.365624188836136, 'R2': -0.4493539880303308, 'Accuracy': 0.6715242191500256, 'Correlation': 0.6844155329498589}\n"
     ]
    }
   ],
   "source": [
    "x = np.array(trans_reps)\n",
    "y = subj1_lang\n",
    "decoding_scores = decoding_module(x, y)\n",
    "print(decoding_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print((summ_reps[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./models/summ_pt.pt\"\n",
    "summ_test = torch.load(path)\n",
    "qna_path = \"./models/qna-fine-tuned.pt\"\n",
    "qna_test = torch.load(qna_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \" I hope this works\"\n",
    "sent2 = \" I just want to go home as soon as possible\"\n",
    "list_s = [sent, sent2]\n",
    "max_length = 20\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "fin_in = []\n",
    "for sent in list_s:\n",
    "    input_s = tokenizer.encode(sent,return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    prompt = [0, 1, 2]\n",
    "    # convert to tensor\n",
    "    prompt = torch.tensor(prompt)\n",
    "    # add batch dimension 1\n",
    "    prompt = prompt.unsqueeze(0)\n",
    "    input_s = torch.cat((prompt, input_s), 1)\n",
    "    fin_in.append(input_s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_in1 = fin_in\n",
    "output = summ_test(fin_in[1])\n",
    "# print device of model and fin_in\n",
    "qna_test = qna_test.to(\"cpu\")\n",
    "output1 = qna_test(fin_in[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([1, 23, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(output[1]))\n",
    "summ_row = 0\n",
    "for row in output[1]:\n",
    "    for loc_row in row:\n",
    "        # shape of loc row is 1, a, b, c\n",
    "        # reshape it to 1, b, a*c\n",
    "        reshaped = loc_row.reshape(1, loc_row.shape[2], loc_row.shape[1]*loc_row.shape[3])\n",
    "        #print(summ_row)\n",
    "        if type(summ_row) == int:\n",
    "            summ_row = reshaped\n",
    "        else:\n",
    "            summ_row = summ_row + reshaped\n",
    "print(summ_row.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([1, 23, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(output[1]))\n",
    "trans_row = 0\n",
    "output = output1\n",
    "for row in output[1]:\n",
    "    for loc_row in row:\n",
    "        # shape of loc row is 1, a, b, c\n",
    "        # reshape it to 1, b, a*c\n",
    "        reshaped = loc_row.reshape(1, loc_row.shape[2], loc_row.shape[1]*loc_row.shape[3])\n",
    "        #print(summ_row)\n",
    "        if type(trans_row) == int:\n",
    "            trans_row = reshaped\n",
    "        else:\n",
    "            trans_row = trans_row + reshaped\n",
    "            trans_row = trans_row + reshaped\n",
    "print(trans_row.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -5.6148,  -2.3915,  -6.4956,  ...,  -6.9506, -29.2854,  38.3489],\n",
      "         [-26.6708, -14.1379, -10.3205,  ...,  -1.1012,  -7.7927,  -3.1892],\n",
      "         [  1.2218,  -1.7441,   1.6339,  ...,  -6.6021,  32.3533,  16.4269],\n",
      "         ...,\n",
      "         [ 19.2474,  -2.5872, -12.3756,  ...,   5.8171, -22.8726,  14.5963],\n",
      "         [ -1.7809,  -2.5101,   0.8455,  ...,  24.0088,  23.7718,  16.6186],\n",
      "         [  8.2050,  -3.4738,   7.5658,  ...,   7.9516,   7.3163,  11.7491]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(trans_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -3.4352,   0.1803,  -2.4653,  ...,   0.0648, -14.0649,  14.0209],\n",
      "         [-14.6782,  -1.5560,   1.7616,  ...,  -1.0546,  -3.2234,  -1.3001],\n",
      "         [ -1.4422,  -2.0077,  -3.2644,  ...,  -4.5564,  15.1565,   9.5084],\n",
      "         ...,\n",
      "         [  5.4493,   1.9400,   1.8145,  ...,   0.2152,  -3.2449,   5.9530],\n",
      "         [  1.8593,   1.7305,   2.3461,  ...,   2.4963,   4.0547,   9.8070],\n",
      "         [  7.6833,   1.3099,  -2.8912,  ...,  10.4999,  17.7710,   2.8434]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(summ_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
